{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/milesturpin/Dev/latent_variable_nn')\n",
    "sys.path.insert(1, '/Users/milesturpin/Dev/latent_variable_nn/models')\n",
    "\n",
    "#from models.multilevel_layers import MultilevelDense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from functools import reduce\n",
    "import ipdb\n",
    "\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(gid):\n",
    "    #x_tf = tf.cast(tf.convert_to_tensor(gid), tf.int32)\n",
    "    gid_tf = tf.cast(tf.convert_to_tensor(gid), tf.int32)\n",
    "    num_groups = tf.cast(tf.convert_to_tensor(max(gid) + 1), tf.int32)\n",
    "    \"\"\"Creates a joint distribution for the varying slope model.\"\"\"\n",
    "    return tfd.JointDistributionSequential([\n",
    "        tfd.Normal(loc=1., scale=0.1, name='w_z_0'),\n",
    "#         tfd.Independent(\n",
    "#             tfd.HalfCauchy(loc=tf.ones([num_groups])*0.1, scale=0.01, name='w_lambda_k'),\n",
    "#             reinterpreted_batch_ndims=1),\n",
    "        tfd.HalfCauchy(loc=1, scale=0.01, name='w_lambda_k'),\n",
    "\n",
    "        lambda w_lambda_k, w_z_0: tfd.Independent(tfd.Normal( \n",
    "            loc=tf.ones([num_groups])*w_z_0,\n",
    "            scale=w_lambda_k,\n",
    "            name='w_z_k'), reinterpreted_batch_ndims=1),\n",
    "\n",
    "        lambda w_z_k: tfd.MultivariateNormalDiag(  # y\n",
    "            loc=tf.gather(w_z_k, gid_tf, axis=-1) ,\n",
    "            scale_identity_multiplier=0.5,\n",
    "            name='x')\n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng=10\n",
    "avg_samp = 50\n",
    "np.random.seed(35)\n",
    "gi = np.random.choice(ng, size=[ng*avg_samp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({9: 48, 7: 51, 1: 37, 0: 50, 8: 59, 2: 62, 5: 38, 3: 55, 6: 49, 4: 51})"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_process = data_generator(gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(5)\n",
    "gen = gen_process.sample(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.9272164>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.276844>,\n",
       " <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 4.289809  ,  0.56151044, -2.057742  , -1.2611173 ,  0.62570345,\n",
       "         0.6008022 , -2.8858562 ,  0.7954581 ,  0.58332413,  0.34865123],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi=gen[-1].numpy()\n",
    "gen[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = 2.*xi + 1. + np.random.randn(len(xi))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test,  gid_train, gid_test = train_test_split(\n",
    "    xi, yi, gi, stratify=gi, random_state=9, test_size=0.9)\n",
    "tts = [x_train, x_test, y_train, y_test, gid_train, gid_test]\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train)[...,tf.newaxis]\n",
    "x_test = tf.convert_to_tensor(x_test)[...,tf.newaxis]\n",
    "y_train = tf.convert_to_tensor(y_train)[...,tf.newaxis]\n",
    "y_test = tf.convert_to_tensor(y_test)[...,tf.newaxis]\n",
    "gid_train = tf.cast(tf.convert_to_tensor(gid_train), tf.int32)\n",
    "gid_test = tf.cast(tf.convert_to_tensor(gid_test), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_estimator_error(train, test):\n",
    "    return np.abs(np.mean(train) - np.mean(test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_tensor = lambda x: isinstance(x, tf.python.framework.ops.EagerTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(data, gid, idx): \n",
    "    if check_tensor(data):\n",
    "        data = data.numpy()\n",
    "    if check_tensor(gid):\n",
    "        gid = gid.numpy()\n",
    "    return data[np.where(gid == idx)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline estimator: 0.033158585\n"
     ]
    }
   ],
   "source": [
    "print('Baseline estimator:', mean_estimator_error(x_train, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline estimator per group: 0.24614668\n",
      "Baseline estimator per group: 0.023810059\n",
      "Baseline estimator per group: 0.065476894\n",
      "Baseline estimator per group: 0.40621316\n",
      "Baseline estimator per group: 0.31369454\n",
      "Baseline estimator per group: 0.16790938\n",
      "Baseline estimator per group: 0.028253317\n",
      "Baseline estimator per group: 0.367454\n",
      "Baseline estimator per group: 0.0071002245\n",
      "Baseline estimator per group: 0.06261778\n"
     ]
    }
   ],
   "source": [
    "for i in range(ng):\n",
    "    train = get_segments(x_train, gid_train, i)\n",
    "    test = get_segments(x_test, gid_test, i)\n",
    "    print('Baseline estimator per group:', mean_estimator_error(train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvEAAAEvCAYAAADM5J4jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dfbBcdZ3n8c+3+3buvQmSy0OoPBEDWZ5cBpT0CmjpjgYHEJBZp3RxdRxnZqW2di2Do86KrAzjQ+EUlsrWTO0W6qxjDSNFCYrIulEz1MjuYPReIlcgBE1kIE9FeEiAPNzc2/3dP/r2ze1O9+nTnXP69K/7/aqi7u3Tvz7ne5LL9/tJp8+JubsAAAAAhCOXdQEAAAAA2kOIBwAAAAJDiAcAAAACQ4gHAAAAAkOIBwAAAAJDiAcAAAACM5TGTk899VRfvXp1GrsGgOBNTEw87+5Lsq4ja8wKAIgWNS9SCfGrV6/W+Ph4GrsGgOCZ2b9kXUMvYFYAQLSoecHHaQAAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDADGVdAPrPPXte1K3bd2vn1LRWDBd045nL9AdLT275XKf7TON1We0XAELUqCdK0q3bd2vH1LTykkqqvHNYnn3NSfmcPn/2ymN65z17XtR/e2qHXiqV1YmThvL6/Fkr5o6/c2paY/mcjkg6MG+fC80kSQfd57YNm2lq3uN6C0w60vzppucEpIEQj0Tds+dFfWLrszpUrnS5HVPT+sTWZ+eeb/ZcVMOL2mcar2slrf0CQIga9cQbtjwjmWl6NhCXZtfOj+UvlcqVdVLNGz03bHlG08dRz0szJX30yWeUc83tp9EfCA42COtRAV6KDvDV49SfE5AWPk6DRN26ffdcI686VHbdun135HOd7jON17WS1n4BIESNeuK0NBfgo0zPvn7+vo4nwFeV5gX4bqs/JyAtvBOPRO2catw2m21v9Vyn+zye17WS1n4BIERJ9tR+6aP9ch7obbwTj0StGC403R71XKf7TON1raS1XwAIUZI9tV/6aL+cB3obIR6JuvHMZRrNWc220ZzpxjOXRT7X6T7TeF0rae0XAELUqCcWJBXMGr+gbt383nnjmcuURPzNmxLZTyfqzwlICx+nQaKqF/JE3bml3bu6xNlnkq9rJa39AkCImvXE6rZ27k5T/Z670wCtmce48KRdxWLRx8fHE98vAPQDM5tw92LWdWSNWQEA0aLmBR+nAQAAAAJDiAcAAAACQ4gHAAAAAkOIBwAAAAJDiAcAAAACQ4gHAAAAAkOIBwAAAAJDiAcAAAACQ4gHAAAAAkOIBwAAAAJDiAcAAAACQ4gHAAAAAkOIBwAAAAJDiAcAAAACQ4gHAAAAAkOIBwAAAAJDiAcAAAACQ4gHAAAAAhMrxJvZx8zscTN7zMy+bWYjaRcGAAgP8wIAumOo1QIzWyHpo5Je5+6HzOxuSddJ+mbKtQGSpN177tP2bV/S4aldkkySzz03NHSSzj77M1q29Nq29rnlyZu1a9ddkkqS8lq+/Dqdd+5nkywbGDjMC8RR6b//oGovz+cX6pxzPt+wj0f1/zjMFsr9UNuvqxrKj2mmdEDSdN0zhbptOUnleV9rqpg9fl5SSSPDy3Xmmk+0PbeAei1D/Lx1o2Y2LWmhpF3plQQctXvPfXryyZtULh+a3VLbiGdmXtKWLf9VkmI3xMoAuXPeltLcY4I8cNyYF2jq2P4rlUoHtWXLJyXV9vFW/T8O94Md1ypJM6V9TZ6pD/Xluq81Vcx+LUmSDk/t0pNP3iQp/twCGmn5cRp33ynpS5KekbRb0n53/1HahQGStH3bl+Y18Mbcp7V925di77PyDnz87QDiYV6glWZ91r10TB+P0/9DVS4famtuAY20DPFmdpKkayWdIWm5pEVm9oEG6643s3EzG9+7d2/ylWIgHZ7anei6ilKb2wHEEWdeMCsGXfM+W9/H2+vr4en380P64lzYepmk37r7XneflnSvpDfVL3L3O9y96O7FJUuWJF0nBtTI8LJE11Xk29wOIKaW84JZMeia99n6Pt5eXw9Pv58f0hcnxD8j6RIzW2hmJmmdpC3plgVUnLnmE8rlRiPXmBV05ppPxN7n8uXXtbUdQGzMC0Rq1mfN8sf08Tj9P1S53GhbcwtoJM5n4jdJ+o6kRyT9avY1d6RcFyCpctHPued+QSPDy2e3WM3zQ0Mn6bzz/qqti4POO/ezWr78/Tr6jlBey5e/n4tagePEvEArR/vv0V6ezy/Ueefddkwfb9X/4zBb2NHrqobyY6rciaZe/bZc3deaKma/VmbOyPBynXvuF7ioFcfN3Du77VKUYrHo4+Pjie8XAPqBmU24ezHrOrLGrACAaFHzgn+xFQAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAjMUNYFAGmanJzUxo0btX//fi1evFjr1q2TpGO2XXDBBake96yzztKvf/3rrh4zjWMAQC+YnJzUD3/4Qx06dOiY58xM7l7TB+vXV9eMjo5KUsP9NFMoFLRy5Uo9/fTTcveW63O5nMrlsiRpdHRUV155pZ555hlNTEzUvN7MdMopp+iFF16Qu8vMtHbtWl199dVz51zt8c3OkRkwWCzOD2C7isWij4+PJ75foB2Tk5O6//77NT09Pbctn8/L3ecaqlRpyNdcc01iza7Rcet145hJHwPJMbMJdy9mXUfWmBXoxOTkpO677z6VSqWWawuFgi688EJt3rw51vpeVCwWtWrVqqZzpXqOjz76KDOgD0XNCz5Og761cePGYxpeqVSqCfCSND09rY0bN6Z63HrdOGbSxwCAXrBx48bYgXx6eloTExPBBnhJmpiYiJwr1XNkBgweQjz61v79+1NZm9S+unHMJI8BAL2g3b6WxicOusndW55zs3NkBvQ3Qjz61uLFi1NZm9S+unHMJI8BAL2g3b5mZilV0h1m1vKcm50jM6C/EeLRt9atW6dCoVCzLZ/PK5er/bEvFApzF7ymddx63Thm0scAgF6wbt065fP5WGsLhYLWrl0be30vWrt2beRcqZ4jM2DwcHca9K3qxTzdvjtNo+OmfXeaZufKBU0A+k21r7Vzd5pVq1YFf3caSZF3p1m1ahUzYMBwdxoA6DLuTlPBrACAaNydBgAAAOgjhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwQ3EWmdmYpK9LOl+SS/oTd384jYIObH5OL294WqV9U8qPDevEy1dr0RtOa7lGkvbfv03lgzOVmkfzGnvXvzrmtd2uFQAGSa/Ni2brJM1tk81WKskW5OTu0rQff4EmLbx4qU7+/bP04vd+rYM/23PsmoI1P9awSVNHn8udNqLlf/Zv2jo3ZhPQv8y9daMys7+T9JC7f93MFkha6O77mq0vFos+Pj7edjEHNj+nfff+Wj5dPnrsQk5j7z5rrgE1WqO8SWWfa8JzctJJ7zknleYVp1YAaMTMJty9mHUdaWhnXnQ6K6T4PbjhzMhJMpNKCQT1GHKnjaj83OHE9lUN8k3nobs0bxOzCQhX1Lxo+XEaM1ss6a2SviFJ7n4kKsAfj5c3PF3bjCT5dFkvb3g6co1KDQK8JJVV89okxakVAAZJr82LZutUVtcCvKTEAnz9vprOw7pNzCagP8X5TPwZkvZK+l9mttnMvm5mi+oXmdn1ZjZuZuN79+7tqJjSvqmW25utaXefxytOrQAwYFrOiyRmhRS/B/dzT27n3Pr51wEYVHFC/JCkiyT9D3d/g6QDkj5Vv8jd73D3orsXlyxZ0lEx+bHhltubrWl3n8crTq0AMGBazoskZoUUvwf3c09u59z6+dcBGFRxQvwOSTvcfdPs4++o0qQTd+Llq2WF2pKskJu7UKfZGuWtcmFSvZxqXpukOLUCwIDpqXnRbJ1yqsyNLsmdNpLKvprOw7pNzCagP7W8O4277zGzZ83sHHffKmmdpCfSKKZ60U3UVfXN1kjdvTtNnFoBYJD02ryIWjd/W6h3p4lzbswmoH/FvTvN61W5ZdgCSdsl/bG7v9Rs/fHccQAA+l2f350m9rxgVgBAtKh5Ees+8e7+S0l9OXAAAMlhXgBAd/AvtgIAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBGcq6APS3LQ89qIfu+pZeeeF5veaUU/WW6z6o897ytp6ro9HzknqidgBAtPk9fOSEE+QuTb36iiyXk5fLkpnkHnt/lsvpgnVXSJIe/ckPj31tg/3lh4d1/lvXafvmX+iV5/dGH9NMF152pS77j/+5rfME5jNv44c6rmKx6OPj44nvF2HZ8tCD+tEdf62ZI1Nz24YWDOv3rv9IV8NwqzoaPZ8bGpK7y0ulTGtHfzKzCXcvZl1H1pgVSEKjHh6KC9/xToI8IkXNCz5Og9Q8dNe3jmmqM0em9NBd3+qpOho9X56ZqQnw9a8BAPSGRj08FJMb/0/WJSBghHik5pUXnm9re1Z1tFNPt2sHAEQLuS97uZx1CQgYIR6pec0pp7a1Pas62qmn27UDAKKF3JctRwxD5/jpQWrect0HNbRguGbb0ILhuYtGe6WORs/nhoZk+XzT1wAAekOjHh6K6sWzQCe4Ow1SU70ANOs7vLSqo9nzvVA7ACBafQ/n7jQYFNydBgC6jLvTVDArACAad6cBAAAA+gghHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAITOwQb2Z5M9tsZj9IsyAAQLiYFQDQHUNtrF0vaYukE1OqBYF7atMePXzfNr364pROOHlYl167RmdfvDTVY6w+/xQ9/dgLsY4Zp75unAPQ55gVaKjT/vrUpj166O6ndPjAzNGNJskly0le1tz+dm/bp8f/7y55ufG+RhYN6dSVJ2jH1n0124cWmGaOeOXrtEse44RMWnn2mHb/dr9KR459Qc2+TMrlTOVS7bpq/dXjVxWG8/rd/3AO8weRYoV4M1sp6SpJX5D0Z6lWhCA9tWmPHrzzSc0cqXTOV1+c0oN3PilJiTWhRsd47Ke75p6POmac+rpxDkA/Y1agmU7761Ob9mjjt7YcE36rIbsa1l99cUo//uYTLcP34QMzxwR4SXMBeqZBGG/K1XBf9fusrj3mHHS0/vrjTk+V9JO/2yKJ+YPm4n6c5quS/lxSkz/bYtA9fN+2ueZcNXOkrIfv25bqMeo1O2ac+rpxDkCfY1agoU7768P3bWsYfhtqI3+HwMvO/EGkliHezK6W9Jy7T7RYd72ZjZvZ+N69exMrEGF49cWptrYneYw46+LU141zAPoVswJROu2vg95/B/38ES3OO/FvlvQuM3ta0l2S3m5mf1+/yN3vcPeiuxeXLFmScJnodSecPNzW9iSPEWddnPq6cQ5AH2NWoKlO++ug999BP39Eaxni3f1Gd1/p7qslXSfpH939A6lXhqBceu0aDS2o/XEaWpDTpdeuSfUY9ZodM0593TgHoF8xKxCl0/566bVrlMtbvIPEXBYKyxnzB5G4TzwScfbFS/W29587967BCScP623vPzfRC3IaHeP8ty6Pdcw49XXjHABgEHXaX8++eKnWffA8jSyquw/HbGC32RRzwsnDeseHXqfz37p8blsjI4uGtPKcsWO2Dy2wo1/j/mHApJXnjCm/oPELavZlaviHkWqtQ3X7KAznddkfncf8QSRzT/5KkGKx6OPj44nvFwD6gZlNuHsx6zqyxqwAgGhR84J34gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDAEOIBAACAwBDiAQAAgMAQ4gEAAIDADGVdwCDaf//9eu4rX9XM7t0aWrZMp33sBi2+5pq212RdY9RaSZnWDwD9Yq7H7tolmUnukiQbG9Oymz4tSdp1819Ihw51p6BqDfm8VCq19dL82Jhec+UVevWffpr6fMh6jgJpM59tBkkqFos+Pj6e+H77wf7779fuz9wsP3x4bpuNjGjZ5z4711zirMm6xsi1hYLcXZqZyaR+oNeZ2YS7F7OuI2vMitYa9dgauZxULne3qISlMR+ynqNAUqLmBR+n6bLnvvLVY5qxHz6s577y1bbWZF1j5Nrp6ZoAH/V6AEBzjXpsjcADvJTOfMh6jgLdQIjvspndu1tuj7MmTe0cv52aulU/APSLQembSZ9n1nMU6AZCfJcNLVvWcnucNWlq5/jt1NSt+gGgXwxK30z6PLOeo0A3EOK77LSP3SAbGanZZiMjcxeDxl2TdY2RawsFaaj2mulu1g8A/aJRj62RC3+MpzEfsp6jQDdwd5ouq15QE3XFfJw1WdfYam2W9QNAv6jpsdydJras5yjQDdydBgC6jLvTVDArACAad6cBAAAA+gghHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAITMsQb2anm9mDZvaEmT1uZuu7URgAICzMCwDonqEYa2YkfdzdHzGz10iaMLMfu/sTKdeWqge2P6DbH7ldew7s0dJFS7X+ovW66syrOt7fhzd8WD/b87OabWPDY3J3vXzk5USOAQA9Lqh5ETUHOp0RD2x/QF/8+Re1b2qfJKlgBc34jFw+t2ZseEwHjhzQtE9Lkkymi5derMnnJ3Vw5mDD/a45cY2eP/y89h/Zf7ynrSEb0ozPRK4pWKGmPpdr2aJlzDGgh7QM8e6+W9Lu2e9fMbMtklZI6smmHMcD2x/QLf98iw6XDkuSdh/YrVv++RZJ6qg5NQrwkuaaeBLHAIBeF9K8iJoDkjqaEQ9sf0Cf+X+f0XR5em5bNQjPN382SJLLG86Q+ba9vC3y+Xa0CvBSbd3VP4Awx4De0tZn4s1staQ3SNqURjHdcvsjt88156rDpcO6/ZHbO9pfq+abxDEAICS9Pi+i5kCnM+L2R26vCfD9iDkG9I44H6eRJJnZCZLukXSDu7/c4PnrJV0vSatWrUqswDTsObCnre3dODYA9IuoedErs6KTOdCqfw9Kfx+U8wR6Xax34s2soEpDvtPd7220xt3vcPeiuxeXLFmSZI2JW7poaVvbu3FsAOgHreZFr8yKqDnQ6YwYlP4+KOcJ9Lo4d6cxSd+QtMXdv5x+Selbf9F6jeRHaraN5Ee0/qLObqRwydJLYq07nmMAQK8LaV5EzYFOZ8T6i9arkCskXmsvYY4BvSPOO/FvlvSHkt5uZr+c/e+dKdeVqqvOvEq3vOkWLVu0TCbTskXLdMubbun4Qp2vXf61hkF+bHhMixcsTuQYABCAYOZF1BzodEZcdeZV+tybP6ex4bG5bQUryGQ168aGx1Swo2HfZLpk6SVaOLSw6b7XnLhGixcs7vBsaw1Z60/S1tcniTkG9Bhz99ar2lQsFn18fDzx/QJAPzCzCXcvZl1H1pgVABAtal7wL7YCAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgSHEAwAAAIEhxAMAAACBIcQDAAAAgRnKugCgLZN3Sxs/K+3fIS1YKB05KMkly0trPyRd/eV0jrV4pbTuZumC9ya3fwBArTh9d/Ju6bv/SfLSvI0mySvfLlhU+Xb6QJODzFs7Jy+p1GDtrMIiqTwtlY7M7iInrf3jysyJqnnuuWcrc8pL0uLT488T5hAiEOIRjsm7pfs/Kk0fqjw+Mq9Be0ka/0bl+ySCfP2x9j9beSzRQAEgDXH67uTd0r0fbvDieaH8SLPw3mDtnIgALx37BwIvV2bOC7+Rdvy8cc1S7flU/9ARd54wh9ACH6dBODZ+9mgza2bim+kda/pQZTsAIHlx+m6v9eDf/lPzmqNmVpx5whxCC7wTj3Ds39F6jbd4N+V4jxWnBgBA++L03VB6cJw6W61hDqEF3olHOBavbL3G8ukeK04NAID2xem7ofTgxStb19rp86H8GiB1hHiEY93NUmE0es3aD6V3rMJoZTsAIHlx+m6v9eAz/m3zmqNmVpx5whxCC4R4hOOC90rX/PfKlf2yyh0IZJXnLC8V/zS5u9PUH2vx6ZXHXEwEAOmI03cveK/07q81+FtXO/rtgkWVu8k0ZQ22tfhb3MIiKb9g3i5ylZnzR99vXnPN+ehozXHnCXMILZh7o6u0j0+xWPTx8fHE9wsA/cDMJty9mHUdWWNWAEC0qHnBO/EAAABAYAjxAAAAQGAI8QAAAEBgCPEAAABAYAjxAAAAQGAI8QAAAEBgCPEAAABAYAjxAAAAQGAI8QAAAEBgCPEAAABAYAjxAAAAQGAI8QAAAEBgCPEAAABAYAjxAAAAQGAI8QAAAEBgCPEAAABAYAjxAAAAQGAI8QAAAEBgYoV4M7vCzLaa2W/M7FNpFwUACBPzAgC6Y6jVAjPLS/obSe+QtEPSL8zs++7+RNrFAehP39u8U7dt2Kpd+w5p+dioPnn5Ofr9N6zoeF2nNfzl/Y/rpYPTkqSx0YJuede/brn/NGsKHfMCGDzNeuL3Nu/Up++d1MHpsiTJJI0Ucjo0XVbeTCX3ua8rxkb1tnOX6IHJ3XM9ebSQU85MB46UOq5tfl+v1rlz3yHlTCp7+/tbOFt/9TwlRc6RtOeFuUefhZldKukWd7989vGNkuTutzZ7TbFY9PHx8cSKBNA/vrd5p26891c6NH20MY8W8rr13b9T09ziruu0hk9+51FNl2r7XyFnuu09Fzbdf1I1mdmEuxc7q753tTsvmBVA2Jr1xD9Yu0L/sOmZjoJy0go5079/4+m6Z2JnTZ1J7LcsqVRuPEckpT4v4nycZoWkZ+c93jG7DQDadtuGrcc00kPTJd22YWtH6zqtoT7AS9J02SP3n2ZNfYJ5AQyQZj3x25ue7YkAL1X6+rc3PZtogK/utz7AV7fftmFrV+ZFy4/TxGVm10u6XpJWrVqV1G4B9Jld+w7F2h53XZI1dPpcEjUNCmYF0D+a9b5Si095dFu36+l0xrQrzjvxOyWdPu/xytltNdz9DncvuntxyZIlSdUHoM8sHxuNtT3uuiRr6PS5JGrqEy3nBbMC6B/Nel/erMuVROt2PcvHRrsyL+KE+F9IOsvMzjCzBZKuk/T9xCoAMFA+efk5Gi3ka7aNFvJzFwm1u67TGgr5Y5t6IWeR+0+zpj7BvAAGSLOe+L6LT1euR3J8IWd638WnH1NnEvvNNzjJ6hzpxrxo+XEad58xs49I2iApL+lv3f3xxCoAMFCqF/S0umI/7rrjqaHdu9OkWVM/YF4AgyWqJxZfe3JP3Z2m+NqTu353mma/NklpeXeaTnDHAQBorl/vTtMuZgUARDveu9MAAAAA6CGEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwhHgAAAAgMIR4AAAAIDCEeAAAACAwqfxjT2a2V9K/JL7j5k6V9HwXjxcXdbWHutpDXe3ppbpe6+5Lsi4iaynMil76Pe42zn0wDfK5S4Nx/k3nRSohvtvMbLwX//VD6moPdbWHutrTq3UhOYP8e8y5c+6DaNDPn4/TAAAAAIEhxAMAAACB6ZcQf0fWBTRBXe2hrvZQV3t6tS4kZ5B/jzn3wTTI5y4N+Pn3xWfiAQAAgEHSL+/EAwAAAAOjb0K8mX3OzCbN7Jdm9iMzW551TZJkZreZ2ZOztX3XzMayrkmSzOw9Zva4mZXNLNMru83sCjPbama/MbNPZVnLfGb2t2b2nJk9lnUtVWZ2upk9aGZPzP7+rc+6JkkysxEz+7mZPTpb119mXdN8ZpY3s81m9oOsa0F3mNnHzczN7NSsa+mWXp03aerV+ZG2Xp0F3URf76MQL+k2d7/A3V8v6QeSbs66oFk/lnS+u18g6SlJN2ZcT9Vjkt4t6adZFmFmeUl/I+lKSa+T9D4ze12WNc3zTUlXZF1EnRlJH3f310m6RNJ/6ZFfrylJb3f3CyW9XtIVZnZJxjXNt17SlqyLQHeY2emSfk/SM1nX0mW9Om9S0ePzI229Ogu6aeD7et+EeHd/ed7DRZJ64sP+7v4jd5+ZffgzSSuzrKfK3be4+9as65D0Rkm/cfft7n5E0l2Srs24JkmSu/9U0otZ1zGfu+9290dmv39FlQa2ItuqJK94dfZhYfa/nvh/0MxWSrpK0tezrgVd8xVJf64e+Rnsll6dNynq2fmRtl6dBd1CX6/omxAvSWb2BTN7VtL71TvvxM/3J5J+mHURPWaFpGfnPd6hAWpEx8PMVkt6g6RN2VZSMftXm7+U9JykH7t7T9Ql6auqBLpy1oUgfWZ2rc5qEtAAAAJZSURBVKSd7v5o1rVkbBDmDfNDvTcLuoS+Lmko6wLaYWY/kbS0wVM3uft97n6TpJvM7EZJH5H0F71Q1+yam1T56687u1FT3LoQJjM7QdI9km6o+1uozLh7SdLrZz+H+10zO9/dM72ewMyulvScu0+Y2e9mWQuSE9XbJH1alY/S9KVenTfIRi/OgrTR148KKsS7+2Uxl94p6X+rSyG+VV1m9iFJV0ta5128p2cbv15Z2inp9HmPV85uQxNmVlClad/p7vdmXU89d99nZg+qcj1B1hcFv1nSu8zsnZJGJJ1oZn/v7h/IuC4ch2a9zcx+R9IZkh41M6nSTx4xsze6+54ulpiaXp03GRno+dHrsyBF9PVZffNxGjM7a97DayU9mVUt85nZFar8lc+73P1g1vX0oF9IOsvMzjCzBZKuk/T9jGvqWVZJJt+QtMXdv5x1PVVmtqR6JwwzG5X0DvXA/4PufqO7r3T31ar8bP3jIDb6QeHuv3L309x99ezv+Q5JF/VLgG9lAOfNwM6PXp0F3UBfP6pvQrykL5rZY2Y2qcpfpfbK7Zb+WtJrJP149vaX/zPrgiTJzP6dme2QdKmkB8xsQxZ1zF6E9RFJG1S5MOdud388i1rqmdm3JT0s6Rwz22Fmf5p1Taq8A/GHkt4++/P0y9l3I7K2TNKDs////UKVz8QP7G2/gIz05LxJSy/Pjy7o1VmALuJfbAUAAAAC00/vxAMAAAADgRAPAAAABIYQDwAAAASGEA8AAAAEhhAPAAAABIYQDwAAAASGEA8AAAAEhhAPAAAABOb/AyKJeUHSQZscAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes =plt.subplots(1,2, figsize=(13,5))\n",
    "for i in range(ng):\n",
    "    \n",
    "    axes[0].scatter(get_segments(x_train, gid_train, i), [i]*len(get_segments(x_train, gid_train, i)))\n",
    "\n",
    "    axes[1].scatter(get_segments(x_test, gid_test, i), [i]*len(get_segments(x_test, gid_test, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([ 4.289809  ,  0.56151044, -2.057742  , -1.2611173 ,  0.62570345,\n",
       "        0.6008022 , -2.8858562 ,  0.7954581 ,  0.58332413,  0.34865123],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalVariableLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, variable_shape, num_groups, kl_weight,\n",
    "                 #prior_kl_weight, group_kl_weights, \n",
    "                 **kwargs):\n",
    "        super(HierarchicalVariableLayer, self).__init__(**kwargs)\n",
    "        self.variable_shape = variable_shape\n",
    "        # flatten out input into a vector\n",
    "        if isinstance(variable_shape, list) or isinstance(variable_shape, tuple):\n",
    "            self.units = reduce(lambda x, y: x*y, variable_shape)\n",
    "        else:\n",
    "            self.units = variable_shape\n",
    "        #    self.units = int(variable_shape)\n",
    "        \n",
    "        self.num_groups = num_groups\n",
    "\n",
    "#         self.prior_kl_weight = prior_kl_weight\n",
    "#         self.group_kl_weights = group_kl_weights\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "        \n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # TODO: do something diff for only 1?\n",
    "#         x_input_shape, gid_input_shape = input_shape\n",
    "#         last_dim = x_input_shape[-1]\n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim}),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "        \n",
    "        # Mean of the variational posterior for group latents\n",
    "        self.mu_k = self.add_weight(\n",
    "            shape=(self.num_groups, self.units),\n",
    "            initializer='random_normal', name='mu_k')\n",
    "        # Variance of the variational posterior for group latents\n",
    "        self.sigma_k = self.add_weight(\n",
    "            shape=(self.num_groups, self.units),\n",
    "            initializer=tf.constant_initializer(-4.), name='sigma_k')\n",
    "\n",
    "        # Mean of the variational posterior for group prior z_0 \n",
    "        self.mu0 = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros', name='mu0')\n",
    "        # Variance of the variational posterior for the group mean z_0\n",
    "        self.sigma0 = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=tf.constant_initializer(-4.), name='sigma0')\n",
    "        \n",
    "        # Fixed parameters for hyperprior over z_0 ~ N(0, v^-1)\n",
    "        self.z0_prior_mean = tf.Variable(0., trainable=False, name='z0_prior_mean')\n",
    "        self.z0_prior_variance = tf.Variable(100., trainable=False, name='z0_prior_variance')\n",
    "\n",
    "#         # Mean of the variational posterior for group prior variance tau_k \n",
    "        self.tau_k_mu = self.add_weight(\n",
    "            shape=[self.units],\n",
    "            #shape=(self.num_groups, self.units),\n",
    "            initializer=tf.constant_initializer(1.), name='tau_k_mu')\n",
    "        # Variance of the variational posterior for the group prior variance tau_k\n",
    "        self.tau_k_sigma = self.add_weight(\n",
    "            shape=[self.units],\n",
    "            #shape=(self.num_groups, self.units),\n",
    "            initializer=tf.constant_initializer(-1.), name='tau_k_sigma')        \n",
    "        \n",
    "#         # Fixed parameters for hyperprior over tau_k ~ N(0, tau_0)\n",
    "        self.tau_k_prior_mean = tf.Variable(1., trainable=False, name='tau_k_prior_mean')\n",
    "#         # Fixed hyperprior over tau_k, aka tau_0\n",
    "        self.tau_k_prior_variance = tf.Variable(1., trainable=False, name='tau_k_prior_variance')\n",
    "        \n",
    "        # Fixed parameters for hyperprior over tau_k ~ N(0, tau_0)\n",
    "        #self.tau_k_mu = tf.Variable(1., trainable=False, name='tau_k_mu')\n",
    "        # Fixed hyperprior over tau_k, aka tau_0\n",
    "        #self.tau_k_sigma = tf.Variable(0.1, trainable=False, name='tau_k_sigma')\n",
    "\n",
    "        super(HierarchicalVariableLayer, self).build(input_shape)\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def sample_posterior(self, mu, sigma):\n",
    "        # By sampling after gather, I use different noise for each sample\n",
    "        eps = np.random.randn(*mu.shape)\n",
    "        samp = mu + sigma*eps\n",
    "        return samp\n",
    "\n",
    "    @tf.function\n",
    "    def compute_kl(self, mu1, sigma1, mu2, sigma2):\n",
    "        kl = (\n",
    "            tf.math.log(sigma2/sigma1)\n",
    "            + (sigma1**2 + (mu1-mu2)**2)/(2*sigma2**2)\n",
    "            - 0.5)\n",
    "        return kl\n",
    "    \n",
    "         \n",
    "    @tf.function\n",
    "    def call(self, gid):\n",
    "\n",
    "        #assert len(gid.shape) == 1, \"gid should be flat vector!\"\n",
    "        \n",
    "        var_activation = tf.math.softplus\n",
    "        sigma_k_pos = var_activation(self.sigma_k)\n",
    "        sigma0_pos = var_activation(self.sigma0)\n",
    "        tau_k_sigma_pos = var_activation(self.tau_k_sigma)\n",
    "        \n",
    "        \n",
    "        # KL between var post on z0 and fixed prior over z0        \n",
    "        z0_kl_loss_fn = lambda: tf.multiply(\n",
    "            self.kl_weight,\n",
    "            tf.reduce_sum(self.compute_kl(\n",
    "                self.mu0, \n",
    "                # Changed from sigma0_pos to this to make it work...\n",
    "                var_activation(self.sigma0),\n",
    "                self.z0_prior_mean, self.z0_prior_variance)))\n",
    "\n",
    "        tau_k_kl_loss_fn = lambda: tf.reduce_sum(\n",
    "            tf.multiply(\n",
    "            self.kl_weight,\n",
    "            self.compute_kl(\n",
    "                self.tau_k_mu, var_activation(self.tau_k_sigma),\n",
    "                self.tau_k_prior_mean, self.tau_k_prior_variance)))\n",
    "\n",
    "        def z_k_kl_loss_fn(): \n",
    "            sigma_k_pos = var_activation(self.sigma_k)\n",
    "            tau_k = self.sample_posterior(self.tau_k_mu, var_activation(self.tau_k_sigma))        \n",
    "            tau_k_sq = tau_k ** 2 \n",
    "            z0 = self.sample_posterior(self.mu0, var_activation(self.sigma0))\n",
    "            kl = self.compute_kl(self.mu_k, sigma_k_pos, z0, tau_k_sq)\n",
    "        \n",
    "            return tf.multiply(self.kl_weight, tf.reduce_sum(kl))\n",
    "        \n",
    "        def z_k_kl_loss_fn():\n",
    "            sigma_k_pos = var_activation(self.sigma_k)\n",
    "            tau_k = self.sample_posterior(self.tau_k_mu, var_activation(self.tau_k_sigma))        \n",
    "            tau_k_sq = tau_k ** 2 \n",
    "            sigma0_pos = var_activation(self.sigma0)\n",
    "            #z0 = self.sample_posterior(self.mu0, var_activation(self.sigma0))\n",
    "            \n",
    "#             tf.print(tf.reduce_sum(sigma_k_pos, axis=[-1]) \n",
    "#             , tf.reduce_sum(sigma0_pos, axis=[-1]) ,\n",
    "#                 tf.einsum('kd, kd  -> k', self.mu_k, self.mu_k)\n",
    "#             , tf.einsum('d,d -> ', self.mu0,self.mu0)\n",
    "#             , -2 * tf.einsum('kd,d -> k', self.mu_k, self.mu0))\n",
    "            \n",
    "            d = self.units\n",
    "            z_k_log_prob = (\n",
    "                - 1/2 * d * np.log(2*np.pi) \n",
    "                - 1/2 * d * tf.math.log(tau_k_sq)\n",
    "                - 1/2/tau_k_sq * (\n",
    "                    # NOTE: trace(sigma_k) sums over units\n",
    "                    tf.reduce_sum(sigma_k_pos, axis=[-1]) \n",
    "                    + tf.reduce_sum(sigma0_pos, axis=[-1]) \n",
    "                    + tf.einsum('kd, kd -> k', self.mu_k, self.mu_k)\n",
    "                    + tf.einsum('d, d -> ', self.mu0, self.mu0)\n",
    "                    - 2 * tf.einsum('kd, d -> k', self.mu_k, self.mu0)))\n",
    "            \n",
    "            return -1 * tf.multiply(self.kl_weight, tf.reduce_sum(z_k_log_prob))\n",
    "        \n",
    "#         def z_k_kl_loss_fn():\n",
    "#             z = self.sample_posterior(self.mu_k, var_activation(self.sigma_k))\n",
    "#             tau_k = self.sample_posterior(self.tau_k_mu, var_activation(self.tau_k_sigma))        \n",
    "#             tau_k_sq = tau_k ** 2 \n",
    "#             z0 = self.sample_posterior(self.mu0, var_activation(self.sigma0))\n",
    "#             return tf.reduce_sum(-1 * tfd.Normal(z0, tau_k_sq).log_prob(z))\n",
    "\n",
    "        \n",
    "        # Can access using model.metrics later once I do custom training\n",
    "        self.add_metric(z0_kl_loss_fn(), name='z0_kl_loss', aggregation='mean')\n",
    "        self.add_metric(tau_k_kl_loss_fn(), name='tau_k_kl_loss', aggregation='mean')\n",
    "        self.add_metric(z_k_kl_loss_fn(), name='z_k_kl_loss', aggregation='mean')\n",
    "        \n",
    "        self.add_loss(z0_kl_loss_fn)\n",
    "        self.add_loss(tau_k_kl_loss_fn)\n",
    "        self.add_loss(z_k_kl_loss_fn)\n",
    "        \n",
    "        gather = lambda x: tf.gather(x, gid)\n",
    "        z = self.sample_posterior(gather(self.mu_k), gather(sigma_k_pos))\n",
    "        var = tf.reshape(z, [-1, *self.variable_shape])\n",
    "        \n",
    "        return var\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_gather(x, ind):\n",
    "    return tf.gather(x + 0, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_kl_weights should be [k1, k2, k3] / train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = HierarchicalVariableLayer((50,200), 30, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(num_batches)\n",
    "print(1/num_batches)\n",
    "#print(group_kl_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.10517>"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KL terms\n",
    "tfd.kl_divergence(tfd.Normal(0., tf.math.exp(-4.)), tfd.Normal(0. , 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2.009158, 2.009158, 2.009158], dtype=float32)>"
      ]
     },
     "execution_count": 1013,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KL terms\n",
    "tfd.kl_divergence(tfd.Normal([-1.,  -1., -1.], tf.math.exp(-2.)), tfd.Normal(0. , 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.5898142, 2.0898142, 2.0898142], dtype=float32)>"
      ]
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KL terms\n",
    "tfd.kl_divergence(tfd.Normal([-1.,  2., 2.], tf.nn.softplus(1.)), tfd.Normal(0. , 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_k_kl_loss_fn():\n",
    "#     z = tf.reduce_mean(\n",
    "#         tfd.Normal(tf.convert_to_tensor([1., -1., 1.]), \n",
    "#                    tf.convert_to_tensor([0.1, 0.1, 0.1]))\n",
    "#         .sample(100), axis=0)\n",
    "    mu0 = tf.reduce_mean(tfd.Normal(1., tf.nn.softplus(-2.)).sample(100), axis=0)\n",
    "    z = tf.reduce_mean(\n",
    "        tfd.Normal(\n",
    "            tf.convert_to_tensor([1.]), \n",
    "            tf.convert_to_tensor([2.]))\n",
    "        .sample(100), axis=0)\n",
    "    print(mu0)\n",
    "    print(z)\n",
    "    return -1 * tfd.Normal(mu0, 1).log_prob(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.99928784, shape=(), dtype=float32)\n",
      "tf.Tensor([1.1110892], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.9251883], dtype=float32)>"
      ]
     },
     "execution_count": 1016,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_k_kl_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.291402], dtype=float32)>"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfd.Normal(1, 0.0001).log_prob([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([16., 16., 16.])>"
      ]
     },
     "execution_count": 1018,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muk = tf.convert_to_tensor(np.ones([3,4])*10.)\n",
    "mu0 = tf.convert_to_tensor(np.ones([4])*12.)\n",
    "\n",
    "(tf.einsum('kd, kd  -> k', muk, muk) \n",
    " + tf.einsum('d,d -> ', mu0, mu0)\n",
    "            - 2 * tf.einsum('kd,d -> k', muk, mu0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float64, numpy=\n",
       "array([[10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10.],\n",
       "       [10., 10., 10., 10.]])>"
      ]
     },
     "execution_count": 1019,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "muk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float64, numpy=array([12., 12., 12., 12.])>"
      ]
     },
     "execution_count": 1020,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_k_kl_loss_fn():\n",
    "    #sigma_k_pos = var_activation(self.sigma_k)\n",
    "    #tau_k = self.sample_posterior(self.tau_k_mu, var_activation(self.tau_k_sigma))        \n",
    "#     sigma_k_pos = tf.convert_to_tensor([0.1, 0.1, 0.1])\n",
    "    sigma_k_pos = tf.convert_to_tensor([2.])\n",
    "    tau_k_sq = tf.convert_to_tensor([1.])\n",
    "    sigma0_pos = tf.convert_to_tensor([tf.nn.softplus(-2.)])\n",
    "    #mu_k = tf.convert_to_tensor([1., -1., 1.])\n",
    "    mu_k = tf.convert_to_tensor([1.])\n",
    "    mu0 = tf.convert_to_tensor(1.)\n",
    "    #z0 = self.sample_posterior(self.mu0, var_activation(self.sigma0))\n",
    "\n",
    "#     print(tf.reduce_sum(sigma_k_pos, axis=[-1]) \n",
    "#             , tf.reduce_sum(sigma0_pos, axis=[-1]) ,\n",
    "#         tf.einsum('i, i  -> i', mu_k, mu_k)\n",
    "#             , tf.einsum(', -> ', mu0, mu0)\n",
    "#             ,- 2 * tf.einsum('i, -> i', mu_k, mu0))\n",
    "    d = 1\n",
    "    z_k_log_prob = (\n",
    "        - 1/2 * d * np.log(2*np.pi) \n",
    "        - 1/2 * d * tf.math.log(tau_k_sq)\n",
    "        - 1/2/tau_k_sq * (\n",
    "            tf.reduce_sum(sigma_k_pos, axis=[-1]) \n",
    "            + tf.reduce_sum(sigma0_pos, axis=[-1]) \n",
    "            + tf.einsum('i, i  -> i', mu_k, mu_k)\n",
    "            + tf.einsum(', -> ', mu0, mu0)\n",
    "            - 2 * tf.einsum('i, -> i', mu_k, mu0)))\n",
    "\n",
    "    return -1* z_k_log_prob\n",
    "    #return -1 * tf.multiply(1, tf.reduce_sum(z_k_log_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.9824027], dtype=float32)>"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_k_kl_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_k_kl_loss_fn(): \n",
    "    #tau_k = self.sample_posterior(self.tau_k_mu, var_activation(self.tau_k_sigma))  \n",
    "    #tau_k = tfd.Normal([-1,2,2], 0.1).sample()\n",
    "    #tau_k_sq = tau_k ** 2 \n",
    "    tau_k_sq = 1\n",
    "    z0 = tfd.Normal(1., tf.nn.softplus(0.1)).sample()\n",
    "    z0 = 0.\n",
    "    print(z0, tau_k_sq)\n",
    "    return tfd.kl_divergence(tfd.Normal([-1.,1.,1.], tf.nn.softplus(-2.)), tfd.Normal(z0, tau_k_sq))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([2.0721903, 2.0721903, 2.0721903], dtype=float32)>"
      ]
     },
     "execution_count": 1024,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_k_kl_loss_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=25\n",
    "num_batches = np.ceil(len(x_train) / batch_size)\n",
    "count_dict = Counter(gid_train.numpy())\n",
    "group_train_sizes = np.array([count_dict[i] for i in range(ng)])\n",
    "# I think this is actually not correct\n",
    "#group_kl_weights = group_train_sizes / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "gid = Input(shape=[1], dtype=tf.int32, batch_size=batch_size)\n",
    "x_hier = HierarchicalVariableLayer(\n",
    "    variable_shape=(1,), \n",
    "    num_groups=tf.convert_to_tensor(ng),\n",
    "    # Make sure this prior weight stays updated\n",
    "#     prior_kl_weight=tf.convert_to_tensor(num_batches, dtype=tf.float32),\n",
    "#     group_kl_weights=tf.convert_to_tensor(group_kl_weights, dtype=tf.float32))\n",
    "    kl_weight=tf.convert_to_tensor(1/num_batches, dtype=tf.float32))\n",
    "out = x_hier(gid)\n",
    "model = Model(\n",
    "    inputs=gid, \n",
    "    outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError(\n",
    "    reduction=tf.keras.losses.Reduction.SUM)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), \n",
    "    loss=mse,\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer HierarchicalVariableLayer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 105.0995 - mse: 3.3316 - z0_kl_loss: 4.0324 - tau_k_kl_loss: 0.3758 - z_k_kl_loss: 6.4968\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 90.2470 - mse: 2.8154 - z0_kl_loss: 3.9335 - tau_k_kl_loss: 0.4716 - z_k_kl_loss: 5.5259\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 79.2320 - mse: 2.4474 - z0_kl_loss: 3.8351 - tau_k_kl_loss: 0.5914 - z_k_kl_loss: 4.5975\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 70.2974 - mse: 2.1162 - z0_kl_loss: 3.7383 - tau_k_kl_loss: 0.7327 - z_k_kl_loss: 4.2252\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 66.3367 - mse: 1.8280 - z0_kl_loss: 3.6475 - tau_k_kl_loss: 0.8445 - z_k_kl_loss: 5.8269\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 62.4880 - mse: 1.6195 - z0_kl_loss: 3.5742 - tau_k_kl_loss: 0.8121 - z_k_kl_loss: 6.6133\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 56.4253 - mse: 1.4071 - z0_kl_loss: 3.5134 - tau_k_kl_loss: 0.7219 - z_k_kl_loss: 6.3889\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 52.6110 - mse: 1.2586 - z0_kl_loss: 3.4530 - tau_k_kl_loss: 0.6279 - z_k_kl_loss: 6.4921\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 48.2856 - mse: 1.0713 - z0_kl_loss: 3.3874 - tau_k_kl_loss: 0.5464 - z_k_kl_loss: 6.8177\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 46.0081 - mse: 0.9609 - z0_kl_loss: 3.3159 - tau_k_kl_loss: 0.4803 - z_k_kl_loss: 7.1968\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 42.8228 - mse: 0.8147 - z0_kl_loss: 3.2391 - tau_k_kl_loss: 0.4279 - z_k_kl_loss: 7.5608\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 40.5727 - mse: 0.7079 - z0_kl_loss: 3.1584 - tau_k_kl_loss: 0.3865 - z_k_kl_loss: 7.8931\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 38.8800 - mse: 0.6252 - z0_kl_loss: 3.0752 - tau_k_kl_loss: 0.3536 - z_k_kl_loss: 8.1965\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 37.1732 - mse: 0.5438 - z0_kl_loss: 2.9909 - tau_k_kl_loss: 0.3268 - z_k_kl_loss: 8.4715\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 35.5519 - mse: 0.4674 - z0_kl_loss: 2.9068 - tau_k_kl_loss: 0.3044 - z_k_kl_loss: 8.7220\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 34.4187 - mse: 0.4120 - z0_kl_loss: 2.8241 - tau_k_kl_loss: 0.2849 - z_k_kl_loss: 8.9498\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 33.4780 - mse: 0.3658 - z0_kl_loss: 2.7441 - tau_k_kl_loss: 0.2670 - z_k_kl_loss: 9.1557\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 32.8323 - mse: 0.3326 - z0_kl_loss: 2.6677 - tau_k_kl_loss: 0.2499 - z_k_kl_loss: 9.3406\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 32.2608 - mse: 0.3036 - z0_kl_loss: 2.5961 - tau_k_kl_loss: 0.2330 - z_k_kl_loss: 9.5060\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 31.7827 - mse: 0.2793 - z0_kl_loss: 2.5298 - tau_k_kl_loss: 0.2159 - z_k_kl_loss: 9.6545\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 31.3152 - mse: 0.2565 - z0_kl_loss: 2.4695 - tau_k_kl_loss: 0.1985 - z_k_kl_loss: 9.7839\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 31.0184 - mse: 0.2415 - z0_kl_loss: 2.4153 - tau_k_kl_loss: 0.1810 - z_k_kl_loss: 9.8941\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.8569 - mse: 0.2325 - z0_kl_loss: 2.3673 - tau_k_kl_loss: 0.1637 - z_k_kl_loss: 9.9913\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.5470 - mse: 0.2181 - z0_kl_loss: 2.3253 - tau_k_kl_loss: 0.1468 - z_k_kl_loss: 10.0747\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.4228 - mse: 0.2116 - z0_kl_loss: 2.2887 - tau_k_kl_loss: 0.1308 - z_k_kl_loss: 10.1472\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.3246 - mse: 0.2062 - z0_kl_loss: 2.2570 - tau_k_kl_loss: 0.1161 - z_k_kl_loss: 10.2118\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.2378 - mse: 0.2016 - z0_kl_loss: 2.2296 - tau_k_kl_loss: 0.1029 - z_k_kl_loss: 10.2667\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.2688 - mse: 0.2017 - z0_kl_loss: 2.2060 - tau_k_kl_loss: 0.0913 - z_k_kl_loss: 10.3157\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.2150 - mse: 0.1985 - z0_kl_loss: 2.1857 - tau_k_kl_loss: 0.0815 - z_k_kl_loss: 10.3590\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.2491 - mse: 0.1991 - z0_kl_loss: 2.1682 - tau_k_kl_loss: 0.0734 - z_k_kl_loss: 10.3948\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.1404 - mse: 0.1942 - z0_kl_loss: 2.1531 - tau_k_kl_loss: 0.0669 - z_k_kl_loss: 10.4231\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1800 - mse: 0.1953 - z0_kl_loss: 2.1403 - tau_k_kl_loss: 0.0619 - z_k_kl_loss: 10.4471\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1703 - mse: 0.1945 - z0_kl_loss: 2.1294 - tau_k_kl_loss: 0.0581 - z_k_kl_loss: 10.4664\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1370 - mse: 0.1930 - z0_kl_loss: 2.1204 - tau_k_kl_loss: 0.0554 - z_k_kl_loss: 10.4804\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1561 - mse: 0.1936 - z0_kl_loss: 2.1133 - tau_k_kl_loss: 0.0536 - z_k_kl_loss: 10.4909\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1343 - mse: 0.1928 - z0_kl_loss: 2.1080 - tau_k_kl_loss: 0.0525 - z_k_kl_loss: 10.4964\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1743 - mse: 0.1946 - z0_kl_loss: 2.1044 - tau_k_kl_loss: 0.0519 - z_k_kl_loss: 10.4981\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1738 - mse: 0.1948 - z0_kl_loss: 2.1025 - tau_k_kl_loss: 0.0517 - z_k_kl_loss: 10.4980\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1770 - mse: 0.1952 - z0_kl_loss: 2.1022 - tau_k_kl_loss: 0.0516 - z_k_kl_loss: 10.4943\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1798 - mse: 0.1958 - z0_kl_loss: 2.1035 - tau_k_kl_loss: 0.0516 - z_k_kl_loss: 10.4873\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.1857 - mse: 0.1965 - z0_kl_loss: 2.1062 - tau_k_kl_loss: 0.0516 - z_k_kl_loss: 10.4787\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1046 - mse: 0.1936 - z0_kl_loss: 2.1101 - tau_k_kl_loss: 0.0515 - z_k_kl_loss: 10.4702\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.0759 - mse: 0.1928 - z0_kl_loss: 2.1149 - tau_k_kl_loss: 0.0514 - z_k_kl_loss: 10.4612\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1110 - mse: 0.1946 - z0_kl_loss: 2.1205 - tau_k_kl_loss: 0.0512 - z_k_kl_loss: 10.4515\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1160 - mse: 0.1951 - z0_kl_loss: 2.1265 - tau_k_kl_loss: 0.0510 - z_k_kl_loss: 10.4414\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1452 - mse: 0.1967 - z0_kl_loss: 2.1327 - tau_k_kl_loss: 0.0508 - z_k_kl_loss: 10.4299\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1429 - mse: 0.1971 - z0_kl_loss: 2.1388 - tau_k_kl_loss: 0.0506 - z_k_kl_loss: 10.4183\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1432 - mse: 0.1975 - z0_kl_loss: 2.1447 - tau_k_kl_loss: 0.0505 - z_k_kl_loss: 10.4076\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1284 - mse: 0.1972 - z0_kl_loss: 2.1502 - tau_k_kl_loss: 0.0505 - z_k_kl_loss: 10.3989\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1111 - mse: 0.1966 - z0_kl_loss: 2.1552 - tau_k_kl_loss: 0.0507 - z_k_kl_loss: 10.3924\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.1348 - mse: 0.1979 - z0_kl_loss: 2.1596 - tau_k_kl_loss: 0.0509 - z_k_kl_loss: 10.3826\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1417 - mse: 0.1988 - z0_kl_loss: 2.1634 - tau_k_kl_loss: 0.0512 - z_k_kl_loss: 10.3719\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.0862 - mse: 0.1968 - z0_kl_loss: 2.1666 - tau_k_kl_loss: 0.0515 - z_k_kl_loss: 10.3652\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0924 - mse: 0.1972 - z0_kl_loss: 2.1692 - tau_k_kl_loss: 0.0519 - z_k_kl_loss: 10.3600\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0842 - mse: 0.1970 - z0_kl_loss: 2.1714 - tau_k_kl_loss: 0.0523 - z_k_kl_loss: 10.3563\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1234 - mse: 0.1985 - z0_kl_loss: 2.1731 - tau_k_kl_loss: 0.0526 - z_k_kl_loss: 10.3550\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1247 - mse: 0.1985 - z0_kl_loss: 2.1746 - tau_k_kl_loss: 0.0528 - z_k_kl_loss: 10.3533\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0874 - mse: 0.1972 - z0_kl_loss: 2.1757 - tau_k_kl_loss: 0.0529 - z_k_kl_loss: 10.3501\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1077 - mse: 0.1982 - z0_kl_loss: 2.1767 - tau_k_kl_loss: 0.0529 - z_k_kl_loss: 10.3464\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0931 - mse: 0.1979 - z0_kl_loss: 2.1773 - tau_k_kl_loss: 0.0528 - z_k_kl_loss: 10.3421\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1008 - mse: 0.1984 - z0_kl_loss: 2.1778 - tau_k_kl_loss: 0.0527 - z_k_kl_loss: 10.3393\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1382 - mse: 0.1999 - z0_kl_loss: 2.1781 - tau_k_kl_loss: 0.0526 - z_k_kl_loss: 10.3392\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1162 - mse: 0.1988 - z0_kl_loss: 2.1782 - tau_k_kl_loss: 0.0525 - z_k_kl_loss: 10.3423\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1048 - mse: 0.1982 - z0_kl_loss: 2.1782 - tau_k_kl_loss: 0.0525 - z_k_kl_loss: 10.3442\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1286 - mse: 0.1992 - z0_kl_loss: 2.1780 - tau_k_kl_loss: 0.0524 - z_k_kl_loss: 10.3444\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1203 - mse: 0.1988 - z0_kl_loss: 2.1778 - tau_k_kl_loss: 0.0523 - z_k_kl_loss: 10.3449\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1423 - mse: 0.1996 - z0_kl_loss: 2.1774 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3469\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1302 - mse: 0.1990 - z0_kl_loss: 2.1771 - tau_k_kl_loss: 0.0520 - z_k_kl_loss: 10.3484\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1350 - mse: 0.1992 - z0_kl_loss: 2.1767 - tau_k_kl_loss: 0.0519 - z_k_kl_loss: 10.3486\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.0991 - mse: 0.1977 - z0_kl_loss: 2.1763 - tau_k_kl_loss: 0.0518 - z_k_kl_loss: 10.3501\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.1115 - mse: 0.1982 - z0_kl_loss: 2.1758 - tau_k_kl_loss: 0.0518 - z_k_kl_loss: 10.3509\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1177 - mse: 0.1985 - z0_kl_loss: 2.1754 - tau_k_kl_loss: 0.0517 - z_k_kl_loss: 10.3506\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1219 - mse: 0.1987 - z0_kl_loss: 2.1751 - tau_k_kl_loss: 0.0517 - z_k_kl_loss: 10.3507\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0741 - mse: 0.1968 - z0_kl_loss: 2.1747 - tau_k_kl_loss: 0.0516 - z_k_kl_loss: 10.3511\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0783 - mse: 0.1970 - z0_kl_loss: 2.1744 - tau_k_kl_loss: 0.0517 - z_k_kl_loss: 10.3503\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1357 - mse: 0.1995 - z0_kl_loss: 2.1742 - tau_k_kl_loss: 0.0517 - z_k_kl_loss: 10.3483\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0726 - mse: 0.1971 - z0_kl_loss: 2.1741 - tau_k_kl_loss: 0.0518 - z_k_kl_loss: 10.3473\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0849 - mse: 0.1975 - z0_kl_loss: 2.1740 - tau_k_kl_loss: 0.0519 - z_k_kl_loss: 10.3482\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0833 - mse: 0.1974 - z0_kl_loss: 2.1741 - tau_k_kl_loss: 0.0520 - z_k_kl_loss: 10.3480\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0826 - mse: 0.1973 - z0_kl_loss: 2.1742 - tau_k_kl_loss: 0.0520 - z_k_kl_loss: 10.3486\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.0795 - mse: 0.1971 - z0_kl_loss: 2.1745 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3490\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1242 - mse: 0.1992 - z0_kl_loss: 2.1748 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3456\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0915 - mse: 0.1980 - z0_kl_loss: 2.1751 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3441\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1007 - mse: 0.1983 - z0_kl_loss: 2.1755 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3446\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1178 - mse: 0.1988 - z0_kl_loss: 2.1758 - tau_k_kl_loss: 0.0522 - z_k_kl_loss: 10.3458\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1062 - mse: 0.1983 - z0_kl_loss: 2.1761 - tau_k_kl_loss: 0.0522 - z_k_kl_loss: 10.3463\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0968 - mse: 0.1978 - z0_kl_loss: 2.1764 - tau_k_kl_loss: 0.0522 - z_k_kl_loss: 10.3470\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.0884 - mse: 0.1973 - z0_kl_loss: 2.1766 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3495\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1444 - mse: 0.1992 - z0_kl_loss: 2.1768 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3527\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1139 - mse: 0.1980 - z0_kl_loss: 2.1769 - tau_k_kl_loss: 0.0520 - z_k_kl_loss: 10.3536\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 30.0679 - mse: 0.1962 - z0_kl_loss: 2.1769 - tau_k_kl_loss: 0.0518 - z_k_kl_loss: 10.3526\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.0860 - mse: 0.1971 - z0_kl_loss: 2.1768 - tau_k_kl_loss: 0.0517 - z_k_kl_loss: 10.3507\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.0642 - mse: 0.1965 - z0_kl_loss: 2.1765 - tau_k_kl_loss: 0.0516 - z_k_kl_loss: 10.3483\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1019 - mse: 0.1981 - z0_kl_loss: 2.1762 - tau_k_kl_loss: 0.0516 - z_k_kl_loss: 10.3470\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1250 - mse: 0.1990 - z0_kl_loss: 2.1758 - tau_k_kl_loss: 0.0517 - z_k_kl_loss: 10.3481\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1530 - mse: 0.2000 - z0_kl_loss: 2.1755 - tau_k_kl_loss: 0.0519 - z_k_kl_loss: 10.3490\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 30.1282 - mse: 0.1992 - z0_kl_loss: 2.1752 - tau_k_kl_loss: 0.0520 - z_k_kl_loss: 10.3473\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0969 - mse: 0.1981 - z0_kl_loss: 2.1750 - tau_k_kl_loss: 0.0521 - z_k_kl_loss: 10.3450\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.0943 - mse: 0.1981 - z0_kl_loss: 2.1749 - tau_k_kl_loss: 0.0522 - z_k_kl_loss: 10.3436\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 30.1203 - mse: 0.1991 - z0_kl_loss: 2.1750 - tau_k_kl_loss: 0.0523 - z_k_kl_loss: 10.3435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fca796782d0>"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model.fit(x=gid_train, \n",
    "          y=x_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=100, \n",
    "          #verbose=False,\n",
    "          callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../experiments/parameter_est/' + current_time, \n",
    "                                                    profile_batch='5',\n",
    "                                                    histogram_freq=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-80de23a90097e1c6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-80de23a90097e1c6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=../experiments/parameter_est/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_estimator_error(pred, test):\n",
    "    return np.abs(pred - np.mean(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline estimator per group: 0.24614668\n",
      "Hierarchical estimator per group: 0.4300878\n",
      "Baseline estimator per group: 0.023810059\n",
      "Hierarchical estimator per group: 0.00587973\n",
      "Baseline estimator per group: 0.065476894\n",
      "Hierarchical estimator per group: 0.033417463\n",
      "Baseline estimator per group: 0.40621316\n",
      "Hierarchical estimator per group: 0.4464631\n",
      "Baseline estimator per group: 0.31369454\n",
      "Hierarchical estimator per group: 0.3300223\n",
      "Baseline estimator per group: 0.16790938\n",
      "Hierarchical estimator per group: 0.19196707\n",
      "Baseline estimator per group: 0.028253317\n",
      "Hierarchical estimator per group: 0.18059921\n",
      "Baseline estimator per group: 0.367454\n",
      "Hierarchical estimator per group: 0.2943706\n",
      "Baseline estimator per group: 0.0071002245\n",
      "Hierarchical estimator per group: 0.014726609\n",
      "Baseline estimator per group: 0.06261778\n",
      "Hierarchical estimator per group: 0.032185733\n"
     ]
    }
   ],
   "source": [
    "for i in range(ng):\n",
    "    train = get_segments(x_train, gid_train, i)\n",
    "    test = get_segments(x_test, gid_test, i)\n",
    "    print('Baseline estimator per group:', mean_estimator_error(train, test))\n",
    "    print('Hierarchical estimator per group:', hierarchical_estimator_error(\n",
    "        model.get_weights()[0][i,0], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 3.770542  ],\n",
       "        [ 0.49348873],\n",
       "        [-2.0392046 ],\n",
       "        [-0.9103699 ],\n",
       "        [ 0.36734965],\n",
       "        [ 0.5442452 ],\n",
       "        [-2.7121506 ],\n",
       "        [ 1.2008336 ],\n",
       "        [ 0.4318366 ],\n",
       "        [ 0.2832278 ]], dtype=float32), array([[-5.1138377],\n",
       "        [-5.835897 ],\n",
       "        [-5.6490517],\n",
       "        [-6.784661 ],\n",
       "        [-6.6830845],\n",
       "        [-6.4983187],\n",
       "        [-4.926761 ],\n",
       "        [-5.0217986],\n",
       "        [-6.1543636],\n",
       "        [-4.746539 ]], dtype=float32), array([0.15041177], dtype=float32), array([0.17162338], dtype=float32), array([0.68814236], dtype=float32), array([0.15417106], dtype=float32), 0.0, 100.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.954483\n",
      "0.51141906\n",
      "-2.138099\n",
      "-0.9506198\n",
      "0.38367742\n",
      "0.56830287\n",
      "-2.8644965\n",
      "1.273917\n",
      "0.439463\n",
      "0.31365985\n"
     ]
    }
   ],
   "source": [
    "for i in range(ng):\n",
    "    print(np.mean(get_segments(x_train, gid_train, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14917071"
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(get_segments(x_train, gid_train, i)) for i in range(ng)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.9272164>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.276844>,\n",
       " <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 4.289809  ,  0.56151044, -2.057742  , -1.2611173 ,  0.62570345,\n",
       "         0.6008022 , -2.8858562 ,  0.7954581 ,  0.58332413,  0.34865123],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes =plt.subplots(1,2, figsize=(13,5))\n",
    "for i in range(ng):\n",
    "    \n",
    "    axes[0].scatter(get_segments(x_train, gid_train, i), [i]*len(get_segments(x_train, gid_train, i)))\n",
    "    axes[0].scatter(model.get_weights()[0][i,0], i, c='black')\n",
    "    axes[1].scatter(get_segments(x_test, gid_test, i), [i]*len(get_segments(x_test, gid_test, i)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultilevelDense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, num_groups,\n",
    "                 multilevel_weights=True, \n",
    "                 multilevel_bias=True,\n",
    "                 group_kl_weights=1.,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 **kwargs):\n",
    "        super(MyMultilevelDense, self).__init__(**kwargs)\n",
    "        self.units = int(units)\n",
    "        self.num_groups = num_groups\n",
    "        self.multilevel_weights = multilevel_weights\n",
    "        self.multilevel_bias = multilevel_bias\n",
    "        self.group_kl_weights = group_kl_weights\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        print(input_shape)\n",
    "        x_input_shape, gid_input_shape = input_shape\n",
    "        last_dim = x_input_shape[-1]\n",
    "        print(self.units, last_dim)\n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim}),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "        self.w = HierarchicalVariableLayer(\n",
    "            variable_shape=[self.units, last_dim],\n",
    "            num_groups=self.num_groups, \n",
    "            kl_weight=self.group_kl_weights,\n",
    "            name='kernel')\n",
    "        \n",
    "        #if self.multilevel_bias:\n",
    "        self.b = HierarchicalVariableLayer(\n",
    "            variable_shape=[self.units],\n",
    "            num_groups=self.num_groups, \n",
    "            kl_weight=self.group_kl_weights,\n",
    "            name='bias')\n",
    "        \n",
    "        super(MyMultilevelDense, self).build(input_shape)\n",
    "     \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        x, gid = inputs\n",
    "        #print(x)\n",
    "        batch_size, num_features = x.shape\n",
    "        # Sanity checks\n",
    "        #assert len(x.shape) >= 2, \"Data is incorrect shape!\"\n",
    "        #assert len(gid.shape) == 1, \"gid should be flat vector!\"\n",
    "        \n",
    "        w = self.w(gid)\n",
    "        b = self.b(gid)\n",
    "            \n",
    "        # B: batch size, p: num_features, u: num_units\n",
    "        einsum_matrix_mult = '{},Bp->Bu'.format(\n",
    "            'Bup' if self.multilevel_weights else 'up')\n",
    "        outputs = tf.einsum(einsum_matrix_mult, w, x)\n",
    "\n",
    "        # Sanity checks\n",
    "        target_shape = (batch_size, self.units)\n",
    "        msg = \"output is shape {}, when should be shape {}\".format(outputs.shape, target_shape)\n",
    "        assert outputs.shape == target_shape, msg\n",
    "        assert len(outputs.shape) == 2, \"Output is wrong shape!\"\n",
    "\n",
    "        if self.use_bias:\n",
    "            #outputs = tf.nn.bias_add(outputs, b)\n",
    "            outputs = outputs + b\n",
    "\n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4], [3]]\n",
      "2 4\n"
     ]
    }
   ],
   "source": [
    "ml_dense = MyMultilevelDense(2, 5)\n",
    "ml_dense.build([[3,4], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 516 ms, sys: 7.41 ms, total: 523 ms\n",
      "Wall time: 522 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[ 0.10893169, -0.05610539],\n",
       "       [ 0.12497821, -0.04415904],\n",
       "       [ 0.06597684,  0.02686396]], dtype=float32)>"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#ml_dense([tf.zeros([3], dtype=np.float32), tf.convert_to_tensor([1,1,3])])\n",
    "ml_dense([tf.zeros([3,4], dtype=np.float32), tf.convert_to_tensor([1,1,3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel/mu_k:0                                     (5, 8)\n",
      "kernel/sigma_k:0                                  (5, 8)\n",
      "kernel/mu0:0                                      (8,)\n",
      "kernel/sigma0:0                                   (8,)\n",
      "kernel/tau_k_mu:0                                 (5, 8)\n",
      "kernel/tau_k_sigma:0                              (5, 8)\n",
      "bias/mu_k:0                                       (5, 2)\n",
      "bias/sigma_k:0                                    (5, 2)\n",
      "bias/mu0:0                                        (2,)\n",
      "bias/sigma0:0                                     (2,)\n",
      "bias/tau_k_mu:0                                   (5, 2)\n",
      "bias/tau_k_sigma:0                                (5, 2)\n"
     ]
    }
   ],
   "source": [
    "for w in ml_dense.trainable_weights:\n",
    "    print('{:50}{}'.format(w.name, w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'kernel/mu_k:0' shape=(5, 8) dtype=float32, numpy=\n",
       "array([[ 1.8501416e-03,  1.7296432e-02, -2.0775227e-02, -2.3526764e-02,\n",
       "         2.7267812e-02, -3.7185002e-02, -6.7756720e-02, -1.0620358e-01],\n",
       "       [-2.4482163e-02,  1.0088761e-04,  2.5464961e-02,  5.5881571e-02,\n",
       "         1.1971504e-02, -6.2160981e-03,  4.8162326e-02, -5.9795447e-02],\n",
       "       [ 1.0677747e-02,  5.4472174e-02, -4.6662588e-02, -4.6387635e-02,\n",
       "        -1.3321842e-02,  5.7152171e-02, -1.0693427e-01,  3.9643988e-02],\n",
       "       [-2.4431769e-03, -8.4535154e-03, -4.2101458e-02,  1.2946784e-03,\n",
       "         1.0733221e-02, -2.4324371e-02,  1.2822822e-01,  5.3705242e-02],\n",
       "       [-1.4718369e-03, -7.3936194e-02, -6.3229525e-03,  4.0745344e-03,\n",
       "        -4.0680684e-02, -3.3157025e-02,  1.4526195e-02,  2.7020663e-02]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_dense.w.mu_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.9272164>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.276844>,\n",
       " <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([ 4.289809  ,  0.56151044, -2.057742  , -1.2611173 ,  0.62570345,\n",
       "         0.6008022 , -2.8858562 ,  0.7954581 ,  0.58332413,  0.34865123],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90, 1], [90]]\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "ml_dense = MyMultilevelDense(1, 10)\n",
    "ml_dense.build([[90,1], [90]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 3 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ml_dense([tf.zeros([3], dtype=np.float32), tf.convert_to_tensor([1,1,3])])\n",
    "#ml_dense([x_train, gid_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    MyMultilevelDense(units=1, num_groups=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([50, 1]), TensorShape([50, 1])]\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=[1], dtype=tf.float32, batch_size=50)\n",
    "gid = Input(shape=[1], dtype=tf.int32, batch_size=50)\n",
    "mldense = MyMultilevelDense(units=1, num_groups=10)\n",
    "out = mldense([x,gid])\n",
    "model = Model(\n",
    "    inputs=[\n",
    "        #[...,tf.newaxis]\n",
    "        x, gid], \n",
    "    outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError(\n",
    "    reduction=tf.keras.losses.Reduction.SUM)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=.03), \n",
    "    loss=mse,\n",
    "    metrics=['mse'])\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer MyMultilevelDense has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesturpin/miniconda3/envs/latent2/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: 918.8531 - mse: 16.7631 - z0_kl_loss: 8.1143 - tau_k_kl_loss: 0.7098 - z_k_kl_loss: 11.1830\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 891.7833 - mse: 16.2448 - z0_kl_loss: 8.0845 - tau_k_kl_loss: 0.7336 - z_k_kl_loss: 10.8971\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 861.3592 - mse: 15.6596 - z0_kl_loss: 8.0548 - tau_k_kl_loss: 0.7585 - z_k_kl_loss: 10.6137\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 834.2944 - mse: 15.1418 - z0_kl_loss: 8.0251 - tau_k_kl_loss: 0.7845 - z_k_kl_loss: 10.3208\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 805.2383 - mse: 14.5845 - z0_kl_loss: 7.9955 - tau_k_kl_loss: 0.8116 - z_k_kl_loss: 10.0182WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.125816). Check your callbacks.\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 805.2383 - mse: 14.5845 - z0_kl_loss: 7.9955 - tau_k_kl_loss: 0.8116 - z_k_kl_loss: 10.0182\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 775.8379 - mse: 14.0206 - z0_kl_loss: 7.9658 - tau_k_kl_loss: 0.8398 - z_k_kl_loss: 9.7063\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 753.6207 - mse: 13.6004 - z0_kl_loss: 7.9362 - tau_k_kl_loss: 0.8691 - z_k_kl_loss: 9.3846\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 724.5295 - mse: 13.0430 - z0_kl_loss: 7.9067 - tau_k_kl_loss: 0.8996 - z_k_kl_loss: 9.0526\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 699.5627 - mse: 12.5682 - z0_kl_loss: 7.8772 - tau_k_kl_loss: 0.9312 - z_k_kl_loss: 8.7098\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 673.3148 - mse: 12.0678 - z0_kl_loss: 7.8477 - tau_k_kl_loss: 0.9639 - z_k_kl_loss: 8.3554\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 651.9503 - mse: 11.6650 - z0_kl_loss: 7.8184 - tau_k_kl_loss: 0.9978 - z_k_kl_loss: 7.9883\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 626.2894 - mse: 11.1761 - z0_kl_loss: 7.7891 - tau_k_kl_loss: 1.0329 - z_k_kl_loss: 7.6079\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 603.5198 - mse: 10.7446 - z0_kl_loss: 7.7600 - tau_k_kl_loss: 1.0692 - z_k_kl_loss: 7.2132\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 584.9232 - mse: 10.3961 - z0_kl_loss: 7.7310 - tau_k_kl_loss: 1.1067 - z_k_kl_loss: 6.8029\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 565.9022 - mse: 10.0382 - z0_kl_loss: 7.7023 - tau_k_kl_loss: 1.1455 - z_k_kl_loss: 6.3762\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 543.6844 - mse: 9.6152 - z0_kl_loss: 7.6737 - tau_k_kl_loss: 1.1855 - z_k_kl_loss: 5.9317\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 523.6790 - mse: 9.2346 - z0_kl_loss: 7.6454 - tau_k_kl_loss: 1.2269 - z_k_kl_loss: 5.4684\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 506.5305 - mse: 8.9089 - z0_kl_loss: 7.6175 - tau_k_kl_loss: 1.2697 - z_k_kl_loss: 4.9853\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 487.4288 - mse: 8.5410 - z0_kl_loss: 7.5901 - tau_k_kl_loss: 1.3139 - z_k_kl_loss: 4.4815\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 472.5606 - mse: 8.2539 - z0_kl_loss: 7.5632 - tau_k_kl_loss: 1.3595 - z_k_kl_loss: 3.9568\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 456.8182 - mse: 7.9450 - z0_kl_loss: 7.5371 - tau_k_kl_loss: 1.4066 - z_k_kl_loss: 3.4118\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 442.3278 - mse: 7.6568 - z0_kl_loss: 7.5119 - tau_k_kl_loss: 1.4552 - z_k_kl_loss: 2.8491\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 430.6969 - mse: 7.4224 - z0_kl_loss: 7.4881 - tau_k_kl_loss: 1.5052 - z_k_kl_loss: 2.2734\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 415.7029 - mse: 7.1197 - z0_kl_loss: 7.4661 - tau_k_kl_loss: 1.5566 - z_k_kl_loss: 1.6938\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 400.4399 - mse: 6.8131 - z0_kl_loss: 7.4467 - tau_k_kl_loss: 1.6093 - z_k_kl_loss: 1.1293\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 389.8013 - mse: 6.6015 - z0_kl_loss: 7.4308 - tau_k_kl_loss: 1.6628 - z_k_kl_loss: 0.6126\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 378.8954 - mse: 6.3854 - z0_kl_loss: 7.4199 - tau_k_kl_loss: 1.7163 - z_k_kl_loss: 0.2013\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 367.3468 - mse: 6.1528 - z0_kl_loss: 7.4158 - tau_k_kl_loss: 1.7678 - z_k_kl_loss: -0.0103\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 356.9140 - mse: 5.9332 - z0_kl_loss: 7.4197 - tau_k_kl_loss: 1.8125 - z_k_kl_loss: 0.0955\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 347.4074 - mse: 5.7227 - z0_kl_loss: 7.4318 - tau_k_kl_loss: 1.8412 - z_k_kl_loss: 0.4989\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 339.3297 - mse: 5.5477 - z0_kl_loss: 7.4506 - tau_k_kl_loss: 1.8472 - z_k_kl_loss: 0.7738\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 329.7449 - mse: 5.3620 - z0_kl_loss: 7.4742 - tau_k_kl_loss: 1.8340 - z_k_kl_loss: 0.5881\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 319.8256 - mse: 5.1803 - z0_kl_loss: 7.5013 - tau_k_kl_loss: 1.8086 - z_k_kl_loss: 0.1406\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 310.8655 - mse: 5.0165 - z0_kl_loss: 7.5307 - tau_k_kl_loss: 1.7766 - z_k_kl_loss: -0.2762\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 301.9564 - mse: 4.8470 - z0_kl_loss: 7.5614 - tau_k_kl_loss: 1.7419 - z_k_kl_loss: -0.5342\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 294.0957 - mse: 4.6919 - z0_kl_loss: 7.5926 - tau_k_kl_loss: 1.7079 - z_k_kl_loss: -0.6398\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 287.8035 - mse: 4.5639 - z0_kl_loss: 7.6236 - tau_k_kl_loss: 1.6765 - z_k_kl_loss: -0.6485\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 280.3495 - mse: 4.4105 - z0_kl_loss: 7.6541 - tau_k_kl_loss: 1.6490 - z_k_kl_loss: -0.6147\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 272.9687 - mse: 4.2579 - z0_kl_loss: 7.6836 - tau_k_kl_loss: 1.6260 - z_k_kl_loss: -0.5750\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 267.3648 - mse: 4.1412 - z0_kl_loss: 7.7120 - tau_k_kl_loss: 1.6076 - z_k_kl_loss: -0.5496\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 261.1907 - mse: 4.0138 - z0_kl_loss: 7.7393 - tau_k_kl_loss: 1.5939 - z_k_kl_loss: -0.5492\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 255.7039 - mse: 3.9011 - z0_kl_loss: 7.7654 - tau_k_kl_loss: 1.5846 - z_k_kl_loss: -0.5772\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 249.3891 - mse: 3.7727 - z0_kl_loss: 7.7903 - tau_k_kl_loss: 1.5794 - z_k_kl_loss: -0.6320\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 244.1338 - mse: 3.6664 - z0_kl_loss: 7.8143 - tau_k_kl_loss: 1.5778 - z_k_kl_loss: -0.7085\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 238.5343 - mse: 3.5536 - z0_kl_loss: 7.8373 - tau_k_kl_loss: 1.5793 - z_k_kl_loss: -0.7968\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 233.5639 - mse: 3.4534 - z0_kl_loss: 7.8597 - tau_k_kl_loss: 1.5833 - z_k_kl_loss: -0.8835\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 228.7151 - mse: 3.3549 - z0_kl_loss: 7.8817 - tau_k_kl_loss: 1.5888 - z_k_kl_loss: -0.9528\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 224.0388 - mse: 3.2587 - z0_kl_loss: 7.9034 - tau_k_kl_loss: 1.5950 - z_k_kl_loss: -0.9889\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 219.0849 - mse: 3.1553 - z0_kl_loss: 7.9252 - tau_k_kl_loss: 1.6005 - z_k_kl_loss: -0.9826\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 215.5778 - mse: 3.0794 - z0_kl_loss: 7.9473 - tau_k_kl_loss: 1.6038 - z_k_kl_loss: -0.9360\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 211.2576 - mse: 2.9867 - z0_kl_loss: 7.9699 - tau_k_kl_loss: 1.6033 - z_k_kl_loss: -0.8678\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 206.9898 - mse: 2.8957 - z0_kl_loss: 7.9931 - tau_k_kl_loss: 1.5978 - z_k_kl_loss: -0.8094\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 204.1667 - mse: 2.8353 - z0_kl_loss: 8.0170 - tau_k_kl_loss: 1.5870 - z_k_kl_loss: -0.7876\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 200.1908 - mse: 2.7538 - z0_kl_loss: 8.0413 - tau_k_kl_loss: 1.5711 - z_k_kl_loss: -0.8059\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 196.4498 - mse: 2.6781 - z0_kl_loss: 8.0658 - tau_k_kl_loss: 1.5513 - z_k_kl_loss: -0.8434\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 192.7685 - mse: 2.6034 - z0_kl_loss: 8.0902 - tau_k_kl_loss: 1.5292 - z_k_kl_loss: -0.8743\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 189.8792 - mse: 2.5440 - z0_kl_loss: 8.1143 - tau_k_kl_loss: 1.5064 - z_k_kl_loss: -0.8855\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 186.9229 - mse: 2.4825 - z0_kl_loss: 8.1378 - tau_k_kl_loss: 1.4841 - z_k_kl_loss: -0.8754\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 183.8426 - mse: 2.4180 - z0_kl_loss: 8.1604 - tau_k_kl_loss: 1.4635 - z_k_kl_loss: -0.8503\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 180.5143 - mse: 2.3483 - z0_kl_loss: 8.1820 - tau_k_kl_loss: 1.4450 - z_k_kl_loss: -0.8192\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 177.9568 - mse: 2.2941 - z0_kl_loss: 8.2026 - tau_k_kl_loss: 1.4293 - z_k_kl_loss: -0.7898\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 175.2812 - mse: 2.2379 - z0_kl_loss: 8.2221 - tau_k_kl_loss: 1.4163 - z_k_kl_loss: -0.7662\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 172.3922 - mse: 2.1777 - z0_kl_loss: 8.2405 - tau_k_kl_loss: 1.4059 - z_k_kl_loss: -0.7508\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 169.9563 - mse: 2.1268 - z0_kl_loss: 8.2579 - tau_k_kl_loss: 1.3980 - z_k_kl_loss: -0.7415\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 167.6355 - mse: 2.0785 - z0_kl_loss: 8.2744 - tau_k_kl_loss: 1.3920 - z_k_kl_loss: -0.7358\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 165.3493 - mse: 2.0309 - z0_kl_loss: 8.2900 - tau_k_kl_loss: 1.3874 - z_k_kl_loss: -0.7301\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 163.1107 - mse: 1.9842 - z0_kl_loss: 8.3049 - tau_k_kl_loss: 1.3836 - z_k_kl_loss: -0.7214\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 161.6729 - mse: 1.9534 - z0_kl_loss: 8.3193 - tau_k_kl_loss: 1.3799 - z_k_kl_loss: -0.7089\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 158.5284 - mse: 1.8885 - z0_kl_loss: 8.3331 - tau_k_kl_loss: 1.3755 - z_k_kl_loss: -0.6927\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 156.7357 - mse: 1.8508 - z0_kl_loss: 8.3465 - tau_k_kl_loss: 1.3698 - z_k_kl_loss: -0.6766\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 154.9976 - mse: 1.8145 - z0_kl_loss: 8.3597 - tau_k_kl_loss: 1.3624 - z_k_kl_loss: -0.6658\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 152.8321 - mse: 1.7702 - z0_kl_loss: 8.3725 - tau_k_kl_loss: 1.3530 - z_k_kl_loss: -0.6627\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 151.1642 - mse: 1.7363 - z0_kl_loss: 8.3851 - tau_k_kl_loss: 1.3418 - z_k_kl_loss: -0.6686\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 149.1895 - mse: 1.6966 - z0_kl_loss: 8.3973 - tau_k_kl_loss: 1.3291 - z_k_kl_loss: -0.6809\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 147.3610 - mse: 1.6601 - z0_kl_loss: 8.4092 - tau_k_kl_loss: 1.3154 - z_k_kl_loss: -0.6960\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 145.5882 - mse: 1.6249 - z0_kl_loss: 8.4207 - tau_k_kl_loss: 1.3015 - z_k_kl_loss: -0.7115\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 143.7716 - mse: 1.5887 - z0_kl_loss: 8.4318 - tau_k_kl_loss: 1.2878 - z_k_kl_loss: -0.7267\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 142.5274 - mse: 1.5641 - z0_kl_loss: 8.4423 - tau_k_kl_loss: 1.2750 - z_k_kl_loss: -0.7423\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 140.8628 - mse: 1.5312 - z0_kl_loss: 8.4523 - tau_k_kl_loss: 1.2634 - z_k_kl_loss: -0.7597\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 139.6442 - mse: 1.5073 - z0_kl_loss: 8.4617 - tau_k_kl_loss: 1.2534 - z_k_kl_loss: -0.7806\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 138.4253 - mse: 1.4836 - z0_kl_loss: 8.4706 - tau_k_kl_loss: 1.2449 - z_k_kl_loss: -0.8056\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 136.4723 - mse: 1.4454 - z0_kl_loss: 8.4790 - tau_k_kl_loss: 1.2379 - z_k_kl_loss: -0.8353\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 135.2873 - mse: 1.4227 - z0_kl_loss: 8.4870 - tau_k_kl_loss: 1.2323 - z_k_kl_loss: -0.8693\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 133.6335 - mse: 1.3908 - z0_kl_loss: 8.4946 - tau_k_kl_loss: 1.2278 - z_k_kl_loss: -0.9065\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 132.8374 - mse: 1.3762 - z0_kl_loss: 8.5019 - tau_k_kl_loss: 1.2239 - z_k_kl_loss: -0.9470\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 131.1247 - mse: 1.3434 - z0_kl_loss: 8.5090 - tau_k_kl_loss: 1.2202 - z_k_kl_loss: -0.9898\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 130.0761 - mse: 1.3241 - z0_kl_loss: 8.5158 - tau_k_kl_loss: 1.2164 - z_k_kl_loss: -1.0360\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 128.9356 - mse: 1.3031 - z0_kl_loss: 8.5227 - tau_k_kl_loss: 1.2121 - z_k_kl_loss: -1.0860\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 127.4633 - mse: 1.2757 - z0_kl_loss: 8.5294 - tau_k_kl_loss: 1.2071 - z_k_kl_loss: -1.1405\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 126.2519 - mse: 1.2538 - z0_kl_loss: 8.5362 - tau_k_kl_loss: 1.2013 - z_k_kl_loss: -1.2004\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 125.3152 - mse: 1.2377 - z0_kl_loss: 8.5430 - tau_k_kl_loss: 1.1947 - z_k_kl_loss: -1.2662\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 124.5420 - mse: 1.2251 - z0_kl_loss: 8.5498 - tau_k_kl_loss: 1.1877 - z_k_kl_loss: -1.3366\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 123.1675 - mse: 1.2006 - z0_kl_loss: 8.5566 - tau_k_kl_loss: 1.1805 - z_k_kl_loss: -1.4110\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 122.2483 - mse: 1.1854 - z0_kl_loss: 8.5634 - tau_k_kl_loss: 1.1736 - z_k_kl_loss: -1.4891\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 121.3578 - mse: 1.1709 - z0_kl_loss: 8.5702 - tau_k_kl_loss: 1.1672 - z_k_kl_loss: -1.5709\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 120.3025 - mse: 1.1534 - z0_kl_loss: 8.5771 - tau_k_kl_loss: 1.1617 - z_k_kl_loss: -1.6578\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 119.2733 - mse: 1.1365 - z0_kl_loss: 8.5839 - tau_k_kl_loss: 1.1572 - z_k_kl_loss: -1.7504\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 118.1479 - mse: 1.1180 - z0_kl_loss: 8.5908 - tau_k_kl_loss: 1.1537 - z_k_kl_loss: -1.8493\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 117.0884 - mse: 1.1009 - z0_kl_loss: 8.5977 - tau_k_kl_loss: 1.1513 - z_k_kl_loss: -1.9542\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 116.0231 - mse: 1.0840 - z0_kl_loss: 8.6048 - tau_k_kl_loss: 1.1496 - z_k_kl_loss: -2.0653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fca79ead5d0>"
      ]
     },
     "execution_count": 1097,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "np.random.seed(4)\n",
    "model.fit(x=[x_train, gid_train], \n",
    "          y=(x_train*0.2+4.+ 0.2*np.random.randn(len(x_train))[...,tf.newaxis]), \n",
    "          batch_size=50, \n",
    "          epochs=100, \n",
    "         callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../experiments/reg/' + current_time, \n",
    "                                                    profile_batch='5',\n",
    "                                                    histogram_freq=10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel/mu_k:0\n",
      "[[ 0.83441734]\n",
      " [ 2.089071  ]\n",
      " [-1.0097547 ]\n",
      " [-1.4385335 ]\n",
      " [ 1.9560344 ]\n",
      " [ 1.840174  ]\n",
      " [-0.5332895 ]\n",
      " [ 1.6452571 ]\n",
      " [ 2.0518682 ]\n",
      " [ 1.6336436 ]]\n",
      "\n",
      "\n",
      "kernel/sigma_k:0\n",
      "[[-4.2714143]\n",
      " [-4.354477 ]\n",
      " [-4.2537813]\n",
      " [-4.658683 ]\n",
      " [-4.2990494]\n",
      " [-4.482695 ]\n",
      " [-4.246948 ]\n",
      " [-3.850448 ]\n",
      " [-4.52718  ]\n",
      " [-4.375024 ]]\n",
      "\n",
      "\n",
      "kernel/mu0:0\n",
      "[0.905346]\n",
      "\n",
      "\n",
      "kernel/sigma0:0\n",
      "[-1.3389193]\n",
      "\n",
      "\n",
      "kernel/tau_k_mu:0\n",
      "[0.893217]\n",
      "\n",
      "\n",
      "kernel/tau_k_sigma:0\n",
      "[-0.04631918]\n",
      "\n",
      "\n",
      "bias/mu_k:0\n",
      "[[1.697501 ]\n",
      " [2.0516033]\n",
      " [1.692329 ]\n",
      " [1.9410629]\n",
      " [2.0961673]\n",
      " [2.0109098]\n",
      " [1.8323202]\n",
      " [1.8839426]\n",
      " [2.0744405]\n",
      " [2.1101747]]\n",
      "\n",
      "\n",
      "bias/sigma_k:0\n",
      "[[-5.561151 ]\n",
      " [-5.4070964]\n",
      " [-5.5803094]\n",
      " [-5.774615 ]\n",
      " [-5.1737623]\n",
      " [-5.1256876]\n",
      " [-5.8266478]\n",
      " [-5.3643117]\n",
      " [-5.3757563]\n",
      " [-5.3647223]]\n",
      "\n",
      "\n",
      "bias/mu0:0\n",
      "[1.9376873]\n",
      "\n",
      "\n",
      "bias/sigma0:0\n",
      "[-4.5012784]\n",
      "\n",
      "\n",
      "bias/tau_k_mu:0\n",
      "[-0.02452884]\n",
      "\n",
      "\n",
      "bias/tau_k_sigma:0\n",
      "[-0.886723]\n",
      "\n",
      "\n",
      "kernel/z0_prior_mean:0\n",
      "0.0\n",
      "\n",
      "\n",
      "kernel/z0_prior_variance:0\n",
      "100.0\n",
      "\n",
      "\n",
      "kernel/tau_k_prior_mean:0\n",
      "1.0\n",
      "\n",
      "\n",
      "kernel/tau_k_prior_variance:0\n",
      "1.0\n",
      "\n",
      "\n",
      "bias/z0_prior_mean:0\n",
      "0.0\n",
      "\n",
      "\n",
      "bias/z0_prior_variance:0\n",
      "100.0\n",
      "\n",
      "\n",
      "bias/tau_k_prior_mean:0\n",
      "1.0\n",
      "\n",
      "\n",
      "bias/tau_k_prior_variance:0\n",
      "1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in model.weights:\n",
    "    print(w.name) \n",
    "    print(w.numpy())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAcZ0lEQVR4nO3dfXBc1Znn8e8jS5ZUtpHH2I7aL6wjBzAMUYDR4ABxzRhtbCZGKAuTjLfIErKporY2VAy7kIorFYYxlSWzMAtsMTspF9kKbLxLKCAximfWsMJUvAPxjIgZhcTGYC9T2JZGfhkL21iyXp79o7uFWnSrX9X39u3fp8pl9emr2w/G/vn4nHPPMXdHREQqX03QBYiISGko0EVEIkKBLiISEQp0EZGIUKCLiEREbVAfvHDhQl+xYkVQHy8iUpHeeOON4+6+KN17gQX6ihUr6OnpCerjRUQqkpn9Y6b3chpyMbP3zOzXZvammX0shS3uv5rZu2bWa2ZXF1OwiIjkL58e+lp3P57hvT8CLk78WA38VeJnEREpk1JNinYCT3vcL4H5ZhYr0b1FRCQHuQa6Ay+Z2Rtmdmea95cC7096fTjRJiIiZZLrkMvn3P2ImS0GXjaz/e7+i3w/LPGXwZ0AF110Ub7fLiIi08iph+7uRxI/DwA/Ba6ZcskRYPmk18sSbVPvs9Xd29y9bdGitKtuRESkQFkD3czmmNm85NfAOuCtKZe9CNyeWO3yWWDQ3ftKXq2IiGSUy5DLJ4Cfmlny+v/p7v/bzP4dgLv/APhr4AvAu8CHwNdmplwRkeD9bO8RHt75NkdPnWPJ/EbuW38pX7wq+GnDrIHu7oeAz6Rp/8Gkrx34RmlLExEJn5/tPcLmF37NuZExAI6cOsfmF34NEHioB/akqIhIJXp459sTYZ50bmSMh3e+PW2gl6NXr0AXEcnD0VPn8mqH8vXqtduiiEgelsxvzKsdpu/Vl5ICXUQkD/etv5TGulkpbY11s7hv/aUZv6eQXn0hFOgiInn44lVLeeiWT7N0fiMGLJ3fyEO3fHraoZNCevWF0Bi6iEievnjV0rzGvu9bf2nKGDpk79UXQoEuIpETtnXiyc/WKhcRkTyEdZ14vr36QmgMXUQipVwrSsJIgS4ikVKuFSVhpEAXkUgp14qSMFKgi0ikFLJOPCo0KSoikVKuFSVhpEAXkcgpx4qSMNKQi4hIRCjQRUQiQoEuIhIRCnQRkYhQoIuIRIQCXUQkIhToIiIRoUAXEYkIBbqISEQo0EVEIkKBLiISEQp0EZGIUKCLiESEdlsUEclD2A6gnkyBLiKSo7AeQJ2kQBepYr29vXR3dzM4OEhTUxPt7e20trYGXVZoTXcAtQJdRALT29tLV1cXIyMjAAwODtLV1QWgUM8g7AdQa1JUpEp1d3dPhHnSyMgI3d3dAVUUfmE/gFqBLlKlBgcH82oPg5/tPcL133+FT357B9d//xV+tvdIWT8/7AdQa8hFpEo1NTWlDe+mpqYAqskuDBOSYT+AWoEukkVUJw7b29tTxtAB6urqaG9vD7CqzMIyIRnmA6gV6CLTiPLEYbL+SvnLKuwTkmGgQBeZxnQTh2ENvny0trZWzH/HkvmNHEkT3mGZkAwDTYqKTKMSJw6jKuwTkmGgQBeZRqYJwrBOHEbZF69aykO3fJql8xsxYOn8Rh665dOhHc8OgoZcRKZRaROHURfmCckwyDnQzWwW0AMccfebprx3B/AwkFwU+oS7P1mqIkUKcWBPP69vP8iZk8PMXVDPtZ0ruWR1c173yDRxWDd4gq3f+BqnTxxn3oULWbPxdi5bs3Ym/jNEcpZPD30TsA+4IMP7P3H3u4ovSaR4B/b0s2vbfkbPjwNw5uQwu7btBygo1CdPHO7bvYuXtj7B6PlhAE4fP8ZLW58AUKhLoHIaQzezZcAGQL1uqQivbz84EeZJo+fHeX37wZy+f9/uXWz9xtf4i40dbP3G19i3e9fEe7ufeXoizD+69zC7n3m6+MJFipDrpOhjwLeA8WmuudXMes3sOTNbnu4CM7vTzHrMrOfYsWP51iqSszMnh/NqnyzZAz99/Bi4T/TAk6F++sTxtN+XqV2kXLIGupndBAy4+xvTXNYFrHD3VuBl4Kl0F7n7Vndvc/e2RYsWFVSwSC7mLqjPq32ybD3weRcuTPt9mdpFyiWXMfTrgZvN7AtAA3CBmf3Y3b+SvMDdT0y6/kngP5e2TJH8XNu5MmUMHaB2dg3Xdq7M+r3ZeuBrNt6eMoYev3c9azbeXnC9Ud1eQMoraw/d3Te7+zJ3XwFsBF6ZHOYAZhab9PJm4pOnIoG5ZHUza29bNdEjn7ugnrW3rcppQjRbD/yyNWtZd+ddzFu4CMyYt3AR6+68q+AJ0eT2AsmHlZLbC/T29hZ0P6leBa9DN7MtQI+7vwh808xuBkaBk8AdpSlPpHCXrG7Oe0UL5NYDv2zN2pKtaIn69gJSPnkFuru/Crya+Pr+Se2bgc2lLEwkKMmg3v3M02VZZ67tBaRU9KSoSBql7IEDPN9/kocO9XFkeISl9XVsbolxa/MCoPL2JZfw0l4uIjPs+f6T3Pv2+xweHsGBw8Mj3Pv2+zzffxKIby9QV1eX8j3aXkAKoUAXmWEPHerj3LintJ0bdx461AfEn0Tt6OiY6JE3NTXR0dGh8XPJm4ZcRGbYkeGRrO2VtC+5hJd66CIzbGl9XV7tIoVSoIvMsM0tMRprLKWtscbY3BLL8B0ihdGQi8gMS65mybTKRaRUFOgiZXBr8wIFuMw4DbmIiESEeugiOejr386hg48wNNxHQ32MlpX3EmvuDLoskRQKdJEs+vq3s3//dxgfPwfA0PBR9u//DoBCXUJFQy4SSc/3n6Tttd8Q2/Umba/9ZuKpzEIcOvjIRJgnjY+f49DBR4otU6Sk1EOXyEk+ap98OjP5qD1Q0MTk0HBfTu37du8q24ZeIumohy6Rk+1R+3w11KdfLz65PduxdSLloECXyMnlUft8tKy8l5qaxpS2mppGWlbeO/FaB0dLGCjQJXCDXV28c0M7+y67nHduaGewq6uo+5X6UftYcyerVn2PhvolgNFQv4RVq76XMiGqg6MlDDSGLoEa7Oqi77v340NDAIwePUrfd+NnpzR1dBR0z80tsZQxdCj+UftYc+e0K1rmXbgwPtySpl2kXNRDl0ANPPrYRJgn+dAQA48+VvA9b21ewCOXLmdZfR0GLKuv45FLl8/ok5prNt5O7ez6lLZiD44WyZd66BKo0b70E5WZ2nNV7kfty31snUg6CnQJVG0sxujRo2nbK02pj60TyZcCXQK1+J67U8bQAayhgcX33F36D+t9Frq3wOBhaFoG7fdD65eLuuWBPf28vv0gZ04OM3dBPdd2ruSS1c0lKlgkPwp0CVRy4nPg0ccY7eujNhZj8T13FzwhmlHvs9D1TRhJPPE5+H78NRQc6gf29LNr235Gz48DcObkMLu27QdQqEsgFOgSuKaOjtIH+FTdWz4K86SRc/H2AgP99e0HJ8I8afT8OK9vP6hAl0Ao0CXSJnZJbP2QhuHfoeX/nSV27PxHFwweLvjeZ04O59UuMtO0bFEiK7lL4tDwUTBjqGEW+y+ZR9+i2R9d1LSs4PvPXVCfV7vITFOgS2Sl3SVxlnHok3PiL+oa4xOjBbq2cyW1s1P/CNXOruHazpUF31OkGBpykcjKuEtifQ00Lc9rlUu6Ay4uWR1/clSrXCQsFOgSWQ31sfhwy9T2hqVwz+6c7zPdAReXrO5UgEtoaMhFIqul7nPUjKVuo1sz5rTUfS6v++iAC6kUCnQJXClPF5os9rddrDpwmoahMXCnYWiMVQdOE/vb/HZzzPWAC5GgachFAlXq04VSDB4mhqcuU4x/Sl63yTh0k+HgC5GgKNAlUNOdLlR0oDctiz8Rmq49g+f7T/LQoT6ODI+wtL6OzS0xrlt5b8oYOnz8gItS6+3tpbu7m8HBQZqammhvb6e1tXXGPk+iQUMuEqhSny6Uov3++NLEyaZZqpj818Lh4RGcj/618Bprsh5wAcS3F3j0Cnhgfvzn3mcLKru3t5euri4GBwcBGBwcpKuri97e3oLuJ9VDPXQJ1NL6Og6nCe9CTxdKkVySmOOGXNP9a6HnuukPuCjlXjHd3d2MjKT+moyMjNDd3a1eukxLgS6BmonThVK0fjnnQC3qXwsl3Csm2TPPtV0kSYEugUqOk08dty5m/DzdQ0DT9q4T5p0/xwezG9O2Z5VpT5gC9oppampKG95NTU1530uqi8bQJXC3Ni+g57rfpW/tlfRc97tFh/nE/i34xENAff3bs37v77/7FrVjoylttWOj/P67b2X/4EwTrQXsFdPe3k5dXeqQU11dHe3t7XnfS6qLAl0ipZiHgNrOn+EP3t7L3KEPwZ25Qx/yB2/vpe38mewfnOcE7HRaW1vp6OiY6JE3NTXR0dGh8XPJSkMuEinFPATU3t7Oh11dXLznyERbXV0d7bns1Z7nBGzW27W2KsAlbzkHupnNAnqAI+5+05T36oGngd8DTgB/4u7vlbBOkZwU8xBQMkALXv+dxwSsyEzIp4e+CdgHXJDmva8D/+zunzKzjcCfA39SgvpE8tJS5ENA6hlLJctpDN3MlgEbgCczXNIJPJX4+jmg3cys+PJE8hNr7sztISCRCMq1h/4Y8C1gXob3lwLvA7j7qJkNAhcCxydfZGZ3AncCXHTRRYXUK5JVrDnLQ0AiEZU10M3sJmDA3d8wsz8s5sPcfSuwFaCtrc2zXC4RdmBPvw6GECmxXIZcrgduNrP3gGeAG8zsx1OuOQIsBzCzWqCJ+OSoyMcc2NPPrm37Jw5TPnNymF3b9nNgT3/AlYlUtqyB7u6b3X2Zu68ANgKvuPtXplz2IvDVxNd/nLhGPXBJ6/XtBxk9P57SNnp+nNe3HwyoIpFoKPjBIjPbYmY3J17+ELjQzN4F/gPw7VIUJ5VhsKuLd25oZ99ll/PODe0Mdk1/gMSZk0MZ2odnojyRqpHXg0Xu/irwauLr+ye1DwFfKmVhUhkGu7ro++79+FA8pEePHqXvu/HfGk1pHsgZ7OqiYXiYofqPP94/d0F9yut0e5MXvUe6SITp0X8pysCjj02EeZIPDTHw6GMZr285uJ2asdTeeM34ea7tXDnxOtPe5KU6nk4kivTovxRltC/9I/XTtTd7/EnOgy03M1y/gPrhk6w81MUlq2+cuG5GTzLKkVbiSKVRoEtRamMxRo9+/FH72lj6R+2T1zcP9NA80PNR+5IlKdfN1ElGuYZ0ciVOcvI2uRIHUKhLaGnIRYqy+J67sYaGlDZraGDxPXcXdX2mE4uKOckon+WSWokjlUiBLkVp6ugg9uCWeA/bjNolS4g9uCXthGg+129uidFYk7p7RLEnGeUT0plW3GgljoSZhlykaE0dHRkDvNDrZ+Iko3xCeu6C+oztImGlQJfQurV5QUknQPMJ6Ws7V6aMoQPUzq5JWYkjEjYacpGqcW3nSmpnp/6WzxTSl6xuZu1tqybCfu6CetbetkoTohJq6qFLZKU7LHrtbatzXop4yepmBbhUFAW6RFLysOjkQRfJw6JXrfoeX/1P2lpXoklDLhJJmQ6L/u1v76Ovf3tAVYnMLAV6yOW78ZXEZT4Ueoz9+7+jUJdIUqCHWHLjq9GjR8F9YuMrhXp20x0KPT5+jkMHHyljNSLloUAPsXw3vpKPtKy8l5qaxozvZ+7Bi1QuBXqI5bvxlXwkeVg0zEr7fsN5oPfZstYkMtMU6CE23QZXkl2suZPLL3/4Yz31mjGn5eAgdH1ToS6RokAPsXw3vpKPS/bUG84buNMwNMaqA6eJHTsPI+ege0vQJYqUjNahh1hyv5OBRx9jtK+P2liMxffcnde+KZVk3+5d7H7maU6fOM68CxeyZuPtXLZmbdH3jTV3EvvlV4E0x9wOHi76/iJhoUAPuXw3vqpU+3bv4qWtTzB6Pr7Xyunjx3hp6xMAJQl1mpbB4Pvp20UiQkMuZbbj0A7WPbeO1qdaWffcOnYc2hF0SaGw+5mnJ8I8afT8MLufebo0H9B+P9RNWfVS1xhvF4kI9dDLaMehHTzw2gMMjcWXIvad7eOB1x4AYEPLhgArC97pE8fzas9b65fjP3dviQ+zNC2Lh3myXSQCFOhl9PivHp8I86ShsSEe/9XjVR/o8y5cyOnjx9K2l0zrlxXgEmkacimTs3sHeOjNf8+OfX/Jj955kD8cbJt4r//sx49AqzZrNt5O7ezUfclrZ9ezZuPtAVUkUnnUQy+Ds3sHOPXCO3xi9EIAPjF6IZv6bgPg1aYemudoi9bkxOdMrHIRqRYK9DL4YOd7+EjqWZYNXs8dA538csFbbLp6U0CVhctla9YqwEWKoCGXMhg7lf4sy0WjC+j8VGfVj5+LSGko0Mtg1vz0Bwsfqz3J9ne3a+miiJSEAr0MLli/AqtL/aUesmF+tHj7xCoXEZFiKdDLYM5Vi5l/y8X8U+0JxnH+qfYEj8e28WpTD6BVLiJSGpoULZM5Vy1m88H/Rt/Zj299q1UuIlIK6qGX0aarN9EwK3X3xIZZDVrlIiIloR56GSVXszz+q8fpP9tP85xmNl29SatcRKQk1EMvsw0tG9h09Saa5zTTf7afx3/1eNGrXHSQtIiAeuhZ7Ti0o6Q96lJv0JU8SDp59mjyIGmgKrbdFZGPqIc+jWT49p3tw/GJ8C2mRz3dBl2F0EHSIpKkQJ9GqcMXMi9RLHTpog6SFpEkBfo0Sh2+kHmJYqFLF3WQtIgkKdCnUerwhdIvXdRB0iKSpECfxkysG9/QsoEHrnuA2JwYhhGbE+OB6x4oeKK1qaOD2INbqF2yBMyoXbKE2INbNCEqUoXMPc1J6JMvMGsAfgHUE18V85y7/+mUa+4AHgaOJJqecPcnp7tvW1ub9/T0FFh2+ZR6lYuISDHM7A13b0v3Xi7LFoeBG9z9jJnVAf/XzP7G3X855bqfuPtdxRYbNhtaNijARaQiZA10j3fhzyRe1iV+TN+tFxGRsstpDN3MZpnZm8AA8LK770lz2a1m1mtmz5nZ8gz3udPMesys59ixjx8ILCIihcsp0N19zN2vBJYB15jZFVMu6QJWuHsr8DLwVIb7bHX3NndvW7RoUTF1i4jIFHmtcnH3U8Au4MYp7SfcPXnO2pPA75WmPBERyVXWQDezRWY2P/F1I/B5YP+UayY/xXIzsK+URYqISHa5rHKJAU+Z2SzifwE86+4/N7MtQI+7vwh808xuBkaBk8AdM1WwiIikl3Ud+kyplHXoIiJhMt06dD0pKiISEQp0EZGIUKCHxI5DO1j33Dpan2pl3XPrij7FSESqj04sCoFSn2IkItVJgR6QyZt+mRnjPp7yfvIgDQW6iOSqYgP97N4BPtj5HmOnhpk1v54L1q9gzlWLgy4rJ1N75JlWGhVzkIaIVJ+KDPSzewc49cI7+Ei8Vzt2aphTL7wDUBGhnu5ou3SKOUhDRKpPRU6KfrDzvYkwT/KRcT7Y+V4wBeUpl553sQdpiEj1qchAHzs1nFd72GTqeddYTUlOMRKR6lSRQy6z5tenDe9Z8+sDqCZ/m67elDKGDvEeuUJcRIpRkT30C9avwOpSS7e6Gi5YvyKYgvJU6nNFRUSgQnvoyYnPSl3lAjraTkRKryIDHeKhnm+AV/JSRxGRbCo20PNV6UsdRUSyqcgx9EJU+lJHEZFsqibQK32po4hINlUT6JmWNFbKUkcRkWyqJtArfamjiEg2VTMpGoWljiIi06maQIfCljqKiFSKqhlyERGJOgW6iEhEKNBFRCJCgS4iEhEKdBGRiKiqVS5J2qRLRKKo6gJdm3SJSFRV3ZCLNukSkaiqukDXJl0iElVVF+japEtEoqrqAl2bdIlIVFXdpKg26RKRqKq6QAdt0iUi0VR1Qy4iIlGlQBcRiQgFuohIRCjQRUQiQoEuIhIRCnQRkYjIGuhm1mBmf2dm/2BmvzGzP0tzTb2Z/cTM3jWzPWa2YiaKFRGRzHLpoQ8DN7j7Z4ArgRvN7LNTrvk68M/u/ingUeDPS1umiIhkkzXQPe5M4mVd4odPuawTeCrx9XNAu5lZyaoUEZGschpDN7NZZvYmMAC87O57plyyFHgfwN1HgUHgwjT3udPMesys59ixY8VVLiIiKXIKdHcfc/crgWXANWZ2RSEf5u5b3b3N3dsWLVpUyC1ERCSDvFa5uPspYBdw45S3jgDLAcysFmgCTpSiQBERyU0uq1wWmdn8xNeNwOeB/VMuexH4auLrPwZecfep4+wiIjKDctltMQY8ZWaziP8F8Ky7/9zMtgA97v4i8EPgf5jZu8BJYOOMVSwiImllDXR37wWuStN+/6Svh4Avlba04p3dO6B9z0WkakR2P/Szewc49cI7EwdCj50a5tQL7wAo1EUkkiL76P8HO9+bCPMkHxnng53vBVOQiMgMi2ygj50azqtdRKTSRTbQZ82vz6tdRKTSRTbQL1i/AqtL/c+zuhouWL8imIJERGZYZCdFkxOfWuUiItUisoEO8VBXgItItYjskIuISLVRoIuIRIQCXUQkIhToIiIRoUAXEYkIBbqISEQo0EVEIsKCOofCzI4B/1jmj10IHC/zZ+YqrLWFtS4Ib21hrQvCW1tY64Lw1fYv3D3tGZ6BBXoQzKzH3duCriOdsNYW1rogvLWFtS4Ib21hrQvCXdtUGnIREYkIBbqISERUW6BvDbqAaYS1trDWBeGtLax1QXhrC2tdEO7aUlTVGLqISJRVWw9dRCSyFOgiIhFRdYFuZg+aWa+ZvWlmL5nZkqBrAjCzh81sf6K2n5rZ/KBrSjKzL5nZb8xs3MwCX75lZjea2dtm9q6ZfTvoepLM7L+b2YCZvRV0LZOZ2XIz22Vmv038f9wUdE1JZtZgZn9nZv+QqO3Pgq5pMjObZWZ7zeznQdeSi6oLdOBhd2919yuBnwP3B11QwsvAFe7eChwANgdcz2RvAbcAvwi6EDObBfwl8EfA5cC/NrPLg61qwo+AG4MuIo1R4D+6++XAZ4FvhOjXbBi4wd0/A1wJ3Ghmnw24psk2AfuCLiJXVRfo7v7BpJdzgFDMCrv7S+4+mnj5S2BZkPVM5u773P3toOtIuAZ4190Puft54BmgM+CaAHD3XwAng65jKnfvc/dfJb4+TTyglgZbVZzHnUm8rEv8CMWfSTNbBmwAngy6llxVXaADmNn3zOx94DbC00Of7N8CfxN0ESG1FHh/0uvDhCScKoGZrQCuAvYEW8lHEsMabwIDwMvuHpbaHgO+BYwHXUiuIhnoZvZ/zOytND86Adz9O+6+HNgG3BWWuhLXfIf4P5G3lauuXGuTymZmc4Hngbun/Es1UO4+lhgCXQZcY2ZXBF2Tmd0EDLj7G0HXko9IHhLt7v8yx0u3AX8N/OkMljMhW11mdgdwE9DuZX5AII9fs6AdAZZPer0s0SbTMLM64mG+zd1fCLqedNz9lJntIj4PEfTE8vXAzWb2BaABuMDMfuzuXwm4rmlFsoc+HTO7eNLLTmB/ULVMZmY3Ev/n3c3u/mHQ9YTY3wMXm9knzWw2sBF4MeCaQs3MDPghsM/d/0vQ9UxmZouSK7rMrBH4PCH4M+num919mbuvIP577JWwhzlUYaAD308MJfQC64jPYofBE8A84OXEksofBF1Qkpn9KzM7DFwL7DCznUHVkpg4vgvYSXxy71l3/01Q9UxmZv8LeB241MwOm9nXg64p4Xrg3wA3JH5vvZnoeYZBDNiV+PP498TH0CtiiWAY6dF/EZGIqMYeuohIJCnQRUQiQoEuIhIRCnQRkYhQoIuIRIQCXUQkIhToIiIR8f8BKtImWWy7ZiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "for i in range(ng):\n",
    "    tmp = get_segments(x_train, gid_train, i)\n",
    "    plt.scatter(tmp,tmp*0.2+4.+0.2*np.random.randn(len(tmp))[...,tf.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6269c1dfe7ef4164\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6269c1dfe7ef4164\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=../experiments/reg/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
