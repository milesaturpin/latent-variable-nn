{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/milesturpin/Dev/latent_variable_nn')\n",
    "sys.path.insert(1, '/Users/milesturpin/Dev/latent_variable_nn/models')\n",
    "\n",
    "#from models.multilevel_layers import MultilevelDense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from functools import reduce\n",
    "import ipdb\n",
    "\n",
    "\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(gid):\n",
    "    #x_tf = tf.cast(tf.convert_to_tensor(gid), tf.int32)\n",
    "    gid_tf = tf.cast(tf.convert_to_tensor(gid), tf.int32)\n",
    "    num_groups = tf.cast(tf.convert_to_tensor(max(gid) + 1), tf.int32)\n",
    "    \"\"\"Creates a joint distribution for the varying slope model.\"\"\"\n",
    "    return tfd.JointDistributionSequential([\n",
    "        tfd.Normal(loc=1., scale=0.1, name='w_z_0'),\n",
    "#         tfd.Independent(\n",
    "#             tfd.HalfCauchy(loc=tf.ones([num_groups])*0.1, scale=0.01, name='w_lambda_k'),\n",
    "#             reinterpreted_batch_ndims=1),\n",
    "        tfd.HalfCauchy(loc=1, scale=0.01, name='w_lambda_k'),\n",
    "\n",
    "        lambda w_lambda_k, w_z_0: tfd.Independent(tfd.Normal( \n",
    "            loc=tf.ones([num_groups])*w_z_0,\n",
    "            scale=w_lambda_k,\n",
    "            name='w_z_k'), reinterpreted_batch_ndims=1),\n",
    "\n",
    "        lambda w_z_k: tfd.MultivariateNormalDiag(  # y\n",
    "            loc=tf.gather(w_z_k, gid_tf, axis=-1) ,\n",
    "            scale_identity_multiplier=0.5,\n",
    "            name='x')\n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng=3\n",
    "avg_samp = 30\n",
    "np.random.seed(35)\n",
    "gi = np.random.choice(ng, size=[ng*avg_samp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 28, 0: 36, 2: 26})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_process = data_generator(gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(4)\n",
    "gen = gen_process.sample(seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.9806732>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0385405>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.16555518, 0.99236894, 2.2233448 ], dtype=float32)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi=gen[-1].numpy()\n",
    "gen[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = 2.*xi + 1. + np.random.randn(len(xi))/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test,  gid_train, gid_test = train_test_split(\n",
    "    xi, yi, gi, stratify=gi, random_state=9, test_size=0.9)\n",
    "tts = [x_train, x_test, y_train, y_test, gid_train, gid_test]\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train)[...,tf.newaxis]\n",
    "x_test = tf.convert_to_tensor(x_test)[...,tf.newaxis]\n",
    "y_train = tf.convert_to_tensor(y_train)[...,tf.newaxis]\n",
    "y_test = tf.convert_to_tensor(y_test)[...,tf.newaxis]\n",
    "gid_train = tf.cast(tf.convert_to_tensor(gid_train), tf.int32)\n",
    "gid_test = tf.cast(tf.convert_to_tensor(gid_test), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_estimator_error(train, test):\n",
    "    return np.abs(np.mean(train) - np.mean(test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_tensor = lambda x: isinstance(x, tf.python.framework.ops.EagerTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments(data, gid, idx): \n",
    "    if check_tensor(data):\n",
    "        data = data.numpy()\n",
    "    if check_tensor(gid):\n",
    "        gid = gid.numpy()\n",
    "    return data[np.where(gid == idx)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline estimator: 0.263757\n"
     ]
    }
   ],
   "source": [
    "print('Baseline estimator:', mean_estimator_error(x_train, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline estimator per group: 0.1178029\n",
      "Baseline estimator per group: 0.21350116\n",
      "Baseline estimator per group: 0.5193312\n"
     ]
    }
   ],
   "source": [
    "for i in range(ng):\n",
    "    train = get_segments(x_train, gid_train, i)\n",
    "    test = get_segments(x_test, gid_test, i)\n",
    "    print('Baseline estimator per group:', mean_estimator_error(train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes =plt.subplots(1,2, figsize=(13,5))\n",
    "for i in range(ng):\n",
    "    \n",
    "    axes[0].scatter(get_segments(x_train, gid_train, i), [i]*len(get_segments(x_train, gid_train, i)))\n",
    "\n",
    "    axes[1].scatter(get_segments(x_test, gid_test, i), [i]*len(get_segments(x_test, gid_test, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.16555518, 0.99236894, 2.2233448 ], dtype=float32)>"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalVariableLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, variable_shape, num_groups, kl_weight,\n",
    "                 #prior_kl_weight, group_kl_weights, \n",
    "                 **kwargs):\n",
    "        super(HierarchicalVariableLayer, self).__init__(**kwargs)\n",
    "        self.variable_shape = variable_shape\n",
    "        # flatten out input into a vector\n",
    "        if isinstance(variable_shape, list) or isinstance(variable_shape, tuple):\n",
    "            self.units = reduce(lambda x, y: x*y, variable_shape)\n",
    "        else:\n",
    "            self.units = variable_shape\n",
    "        #    self.units = int(variable_shape)\n",
    "        \n",
    "        self.num_groups = num_groups\n",
    "\n",
    "#         self.prior_kl_weight = prior_kl_weight\n",
    "#         self.group_kl_weights = group_kl_weights\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "        \n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # TODO: do something diff for only 1?\n",
    "#         x_input_shape, gid_input_shape = input_shape\n",
    "#         last_dim = x_input_shape[-1]\n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim}),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "        \n",
    "        # Mean of the variational posterior for group latents\n",
    "        self.mu_k = self.add_weight(\n",
    "            shape=(self.num_groups, self.units),\n",
    "            initializer='random_normal', name='mu_k')\n",
    "        # Variance of the variational posterior for group latents\n",
    "        self.sigma_k = self.add_weight(\n",
    "            shape=(self.num_groups, self.units),\n",
    "            initializer=tf.constant_initializer(-4.), name='sigma_k')\n",
    "\n",
    "        # Mean of the variational posterior for group prior z_0 \n",
    "        self.mu0 = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer='ones', name='mu0')\n",
    "        # Variance of the variational posterior for the group mean z_0\n",
    "        self.sigma0 = self.add_weight(\n",
    "            shape=(self.units,),\n",
    "            initializer=tf.constant_initializer(-4.), name='sigma0')\n",
    "        \n",
    "        # Fixed parameters for hyperprior over z_0 ~ N(0, v^-1)\n",
    "        self.z0_prior_mean = tf.Variable(0., trainable=False, name='z0_prior_mean')\n",
    "        self.z0_prior_variance = tf.Variable(100., trainable=False, name='z0_prior_variance')\n",
    "\n",
    "        # Mean of the variational posterior for group prior variance tau_k \n",
    "        self.tau_k_mu = self.add_weight(\n",
    "            shape=(self.num_groups, self.units),\n",
    "            initializer=tf.constant_initializer(-1), name='tau_k_mu')\n",
    "        # Variance of the variational posterior for the group prior variance tau_k\n",
    "        self.tau_k_sigma = self.add_weight(\n",
    "            shape=(self.num_groups, self.units),\n",
    "            initializer=tf.constant_initializer(-2), name='tau_k_sigma')\n",
    "        \n",
    "        # Fixed parameters for hyperprior over tau_k ~ N(0, tau_0)\n",
    "        self.tau_k_prior_mean = tf.Variable(0., trainable=False, name='tau_k_prior_mean')\n",
    "        # Fixed hyperprior over tau_k, aka tau_0\n",
    "        self.tau_k_prior_variance = tf.Variable(100., trainable=False, name='tau_k_prior_variance')\n",
    "        \n",
    "        super(HierarchicalVariableLayer, self).build(input_shape)\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def sample_posterior(self, mu, sigma):\n",
    "        # By sampling after gather, I use different noise for each sample\n",
    "        eps = np.random.randn(*mu.shape)\n",
    "        samp = mu + sigma*eps\n",
    "        return samp\n",
    "\n",
    "    @tf.function\n",
    "    def compute_kl(self, mu1, sigma1, mu2, sigma2):\n",
    "        kl = (\n",
    "            tf.math.log(sigma2/sigma1)\n",
    "            + (sigma1**2 + (mu1-mu2)**2)/(2*sigma2**2)\n",
    "            - 0.5)\n",
    "        return kl\n",
    "    \n",
    "         \n",
    "    @tf.function\n",
    "    def call(self, gid):\n",
    "\n",
    "        #assert len(gid.shape) == 1, \"gid should be flat vector!\"\n",
    "        \n",
    "        var_activation = tf.math.exp\n",
    "        sigma_k_pos = var_activation(self.sigma_k)\n",
    "        sigma0_pos = var_activation(self.sigma0)\n",
    "        tau_k_sigma_pos = var_activation(self.tau_k_sigma)\n",
    "        \n",
    "        \n",
    "        # KL between var post on z0 and fixed prior over z0        \n",
    "        z0_kl_loss_fn = lambda: tf.multiply(\n",
    "            self.kl_weight,\n",
    "            tf.reduce_sum(self.compute_kl(\n",
    "                self.mu0, \n",
    "                # Changed from sigma0_pos to this to make it work...\n",
    "                var_activation(self.sigma0),\n",
    "                self.z0_prior_mean, self.z0_prior_variance)))\n",
    "\n",
    "        tau_k_kl_loss_fn = lambda: tf.reduce_sum(\n",
    "            tf.multiply(\n",
    "            self.kl_weight,\n",
    "            self.compute_kl(\n",
    "                self.tau_k_mu, var_activation(self.tau_k_sigma),\n",
    "                self.tau_k_prior_mean, self.tau_k_prior_variance)))\n",
    "\n",
    "        def z_k_kl_loss_fn(): \n",
    "            sigma_k_pos = var_activation(self.sigma_k)\n",
    "            tau_k = self.sample_posterior(self.tau_k_mu, var_activation(self.tau_k_sigma))        \n",
    "            tau_k_sq = tau_k ** 2 \n",
    "            z0 = self.sample_posterior(self.mu0, var_activation(self.sigma0))\n",
    "            kl = self.compute_kl(self.mu_k, sigma_k_pos, z0, tau_k_sq)\n",
    "        \n",
    "            return tf.multiply(self.kl_weight, tf.reduce_sum(kl))\n",
    "        \n",
    "#         def z_k_kl_loss_fn():\n",
    "#             sigma_k_pos = var_activation(self.sigma_k)\n",
    "#             tau_k = self.sample_posterior(self.tau_k_mu, var_activation(self.tau_k_sigma))        \n",
    "#             tau_k_sq = tau_k ** 2 \n",
    "#             sigma0_pos = var_activation(self.sigma0)\n",
    "#             #z0 = self.sample_posterior(self.mu0, var_activation(self.sigma0))\n",
    "            \n",
    "#             d = self.units\n",
    "#             z_k_log_prob = (\n",
    "#                 - 1/2 * d * np.log(2*np.pi) \n",
    "#                 - 1/2 * d * tf.math.log(tau_k_sq)\n",
    "#                 - 1/2/tau_k_sq * (\n",
    "#                     tf.reduce_sum(sigma_k_pos, axis=[-1]) \n",
    "#                     + tf.reduce_sum(sigma0_pos, axis=[-1]) \n",
    "#                     + tf.einsum('Bi, Bi -> ', self.mu_k, self.mu_k)\n",
    "#                     + tf.einsum('i, i -> ', self.mu0, self.mu0)\n",
    "#                     - 2 * tf.einsum('Bi, i -> ', self.mu_k, self.mu0)))\n",
    "            \n",
    "#             return -1 * tf.multiply(self.kl_weight, tf.reduce_sum(z_k_log_prob))\n",
    "\n",
    "        \n",
    "        # Can access using model.metrics later once I do custom training\n",
    "        self.add_metric(z0_kl_loss_fn(), name='z0_kl_loss', aggregation='mean')\n",
    "        #self.add_metric(tau_k_kl_loss_fn(), name='tau_k_kl_loss', aggregation='mean')\n",
    "        self.add_metric(z_k_kl_loss_fn(), name='z_k_kl_loss', aggregation='mean')\n",
    "        \n",
    "        #self.add_loss(lambda: tf.reduce_sum(var_activation(self.sigma0)) )\n",
    "        self.add_loss(z0_kl_loss_fn)\n",
    "        #self.add_loss(tau_k_kl_loss_fn)\n",
    "        self.add_loss(z_k_kl_loss_fn)\n",
    "        \n",
    "        gather = lambda x: tf.gather(x, gid)\n",
    "        z = self.sample_posterior(gather(self.mu_k), gather(sigma_k_pos))\n",
    "        var = tf.reshape(z, [-1, *self.variable_shape])\n",
    "        \n",
    "        return var\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_gather(x, ind):\n",
    "    return tf.gather(x + 0, ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_kl_weights should be [k1, k2, k3] / train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = HierarchicalVariableLayer((50,200), 30, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=9\n",
    "num_batches = np.ceil(len(x_train) / batch_size)\n",
    "count_dict = Counter(gid_train.numpy())\n",
    "group_train_sizes = np.array([count_dict[i] for i in range(ng)])\n",
    "# I think this is actually not correct\n",
    "#group_kl_weights = group_train_sizes / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(num_batches)\n",
    "print(1/num_batches)\n",
    "#print(group_kl_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [],
   "source": [
    "gid = Input(shape=[1], dtype=tf.int32, batch_size=batch_size)\n",
    "x_hier = HierarchicalVariableLayer(\n",
    "    variable_shape=(1,), \n",
    "    num_groups=tf.convert_to_tensor(ng),\n",
    "    # Make sure this prior weight stays updated\n",
    "#     prior_kl_weight=tf.convert_to_tensor(num_batches, dtype=tf.float32),\n",
    "#     group_kl_weights=tf.convert_to_tensor(group_kl_weights, dtype=tf.float32))\n",
    "    kl_weight=tf.convert_to_tensor(1/num_batches, dtype=tf.float32))\n",
    "out = x_hier(gid)\n",
    "model = Model(\n",
    "    inputs=gid, \n",
    "    outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), \n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer HierarchicalVariableLayer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 43.3069 - mse: 2.6679 - z0_kl_loss: 8.1052 - z_k_kl_loss: 12.2143\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 40.4929 - mse: 2.4294 - z0_kl_loss: 8.0052 - z_k_kl_loss: 11.0265\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 38.3552 - mse: 2.2045 - z0_kl_loss: 7.9052 - z_k_kl_loss: 10.1701\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 36.1808 - mse: 2.0349 - z0_kl_loss: 7.8052 - z_k_kl_loss: 9.2678\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 33.7860 - mse: 1.8735 - z0_kl_loss: 7.7052 - z_k_kl_loss: 8.2511\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 31.3621 - mse: 1.7312 - z0_kl_loss: 7.6052 - z_k_kl_loss: 7.2102\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 29.6701 - mse: 1.5780 - z0_kl_loss: 7.5052 - z_k_kl_loss: 6.5409\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 28.5289 - mse: 1.4292 - z0_kl_loss: 7.4051 - z_k_kl_loss: 6.1448\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 26.0141 - mse: 1.4007 - z0_kl_loss: 7.3050 - z_k_kl_loss: 5.0017\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 23.1330 - mse: 1.3750 - z0_kl_loss: 7.2049 - z_k_kl_loss: 3.6741\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 25.0245 - mse: 1.4845 - z0_kl_loss: 7.1047 - z_k_kl_loss: 4.6653\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 19.7712 - mse: 1.5144 - z0_kl_loss: 7.0049 - z_k_kl_loss: 2.1235\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.3235 - mse: 1.6864 - z0_kl_loss: 6.9051 - z_k_kl_loss: 1.4135\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 18.4933 - mse: 1.6669 - z0_kl_loss: 6.8053 - z_k_kl_loss: 1.6079\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.9779 - mse: 1.6823 - z0_kl_loss: 6.7053 - z_k_kl_loss: 0.9425\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.1085 - mse: 1.7699 - z0_kl_loss: 6.6051 - z_k_kl_loss: 0.5641\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.8466 - mse: 1.7052 - z0_kl_loss: 6.5050 - z_k_kl_loss: 1.0657\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.6330 - mse: 1.6302 - z0_kl_loss: 6.4050 - z_k_kl_loss: 1.0964\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 16.2043 - mse: 1.7852 - z0_kl_loss: 6.3052 - z_k_kl_loss: 0.9044\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 15.6661 - mse: 1.5713 - z0_kl_loss: 6.2053 - z_k_kl_loss: 0.8421\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 15.3703 - mse: 1.6495 - z0_kl_loss: 6.1053 - z_k_kl_loss: 0.7551\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.6567 - mse: 1.4989 - z0_kl_loss: 6.0051 - z_k_kl_loss: 0.5738\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 14.0924 - mse: 1.5666 - z0_kl_loss: 5.9049 - z_k_kl_loss: 0.3580\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.6261 - mse: 1.5615 - z0_kl_loss: 5.8046 - z_k_kl_loss: 0.2277\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.3344 - mse: 1.5825 - z0_kl_loss: 5.7043 - z_k_kl_loss: 0.1716\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 13.0848 - mse: 1.7237 - z0_kl_loss: 5.6042 - z_k_kl_loss: 0.0763\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.4577 - mse: 1.3642 - z0_kl_loss: 5.5043 - z_k_kl_loss: 0.0424\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.5287 - mse: 1.4895 - z0_kl_loss: 5.4043 - z_k_kl_loss: 0.1153\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.6632 - mse: 1.7142 - z0_kl_loss: 5.3043 - z_k_kl_loss: 0.1702\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 12.0792 - mse: 1.3287 - z0_kl_loss: 5.2041 - z_k_kl_loss: 0.1711\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.8802 - mse: 1.3444 - z0_kl_loss: 5.1039 - z_k_kl_loss: 0.1640\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.5757 - mse: 1.2460 - z0_kl_loss: 5.0036 - z_k_kl_loss: 0.1612\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.5190 - mse: 1.4035 - z0_kl_loss: 4.9034 - z_k_kl_loss: 0.1543\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.4757 - mse: 1.5862 - z0_kl_loss: 4.8033 - z_k_kl_loss: 0.1415\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 11.0390 - mse: 1.3786 - z0_kl_loss: 4.7033 - z_k_kl_loss: 0.1269\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.7862 - mse: 1.3502 - z0_kl_loss: 4.6034 - z_k_kl_loss: 0.1146\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.4923 - mse: 1.2764 - z0_kl_loss: 4.5038 - z_k_kl_loss: 0.1042\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.6363 - mse: 1.6442 - z0_kl_loss: 4.4042 - z_k_kl_loss: 0.0918\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.2759 - mse: 1.5092 - z0_kl_loss: 4.3048 - z_k_kl_loss: 0.0785\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.1840 - mse: 1.6303 - z0_kl_loss: 4.2054 - z_k_kl_loss: 0.0715\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.0685 - mse: 1.7088 - z0_kl_loss: 4.1060 - z_k_kl_loss: 0.0739\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.6508 - mse: 1.4802 - z0_kl_loss: 4.0065 - z_k_kl_loss: 0.0788\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.4201 - mse: 1.4554 - z0_kl_loss: 3.9069 - z_k_kl_loss: 0.0755\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 9.0502 - mse: 1.3134 - z0_kl_loss: 3.8072 - z_k_kl_loss: 0.0613\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.9654 - mse: 1.4660 - z0_kl_loss: 3.7073 - z_k_kl_loss: 0.0424\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.7605 - mse: 1.4926 - z0_kl_loss: 3.6074 - z_k_kl_loss: 0.0265\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.4494 - mse: 1.3996 - z0_kl_loss: 3.5075 - z_k_kl_loss: 0.0175\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2409 - mse: 1.3950 - z0_kl_loss: 3.4075 - z_k_kl_loss: 0.0155\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.2226 - mse: 1.5703 - z0_kl_loss: 3.3076 - z_k_kl_loss: 0.0185\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 8.0973 - mse: 1.6364 - z0_kl_loss: 3.2078 - z_k_kl_loss: 0.0226\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7753 - mse: 1.5101 - z0_kl_loss: 3.1083 - z_k_kl_loss: 0.0243\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.4930 - mse: 1.4304 - z0_kl_loss: 3.0089 - z_k_kl_loss: 0.0224\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.1577 - mse: 1.2994 - z0_kl_loss: 2.9099 - z_k_kl_loss: 0.0193\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.0727 - mse: 1.4146 - z0_kl_loss: 2.8111 - z_k_kl_loss: 0.0180\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8622 - mse: 1.3983 - z0_kl_loss: 2.7126 - z_k_kl_loss: 0.0194\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.8977 - mse: 1.6235 - z0_kl_loss: 2.6144 - z_k_kl_loss: 0.0226\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.5525 - mse: 1.4680 - z0_kl_loss: 2.5167 - z_k_kl_loss: 0.0256\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.2461 - mse: 1.3531 - z0_kl_loss: 2.4193 - z_k_kl_loss: 0.0272\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 6.1390 - mse: 1.4370 - z0_kl_loss: 2.3225 - z_k_kl_loss: 0.0286\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7859 - mse: 1.2704 - z0_kl_loss: 2.2262 - z_k_kl_loss: 0.0316\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.9255 - mse: 1.5903 - z0_kl_loss: 2.1306 - z_k_kl_loss: 0.0370\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 5.4517 - mse: 1.2905 - z0_kl_loss: 2.0361 - z_k_kl_loss: 0.0445\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.2813 - mse: 1.2900 - z0_kl_loss: 1.9428 - z_k_kl_loss: 0.0529\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.0093 - mse: 1.1855 - z0_kl_loss: 1.8512 - z_k_kl_loss: 0.0607\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.1378 - mse: 1.4792 - z0_kl_loss: 1.7619 - z_k_kl_loss: 0.0674\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6128 - mse: 1.1165 - z0_kl_loss: 1.6753 - z_k_kl_loss: 0.0729\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4412 - mse: 1.1022 - z0_kl_loss: 1.5920 - z_k_kl_loss: 0.0774\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0989 - mse: 0.9122 - z0_kl_loss: 1.5125 - z_k_kl_loss: 0.0809\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.4638 - mse: 1.4250 - z0_kl_loss: 1.4369 - z_k_kl_loss: 0.0825\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.2381 - mse: 1.3420 - z0_kl_loss: 1.3653 - z_k_kl_loss: 0.0827\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.3349 - mse: 1.5771 - z0_kl_loss: 1.2976 - z_k_kl_loss: 0.0813\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7935 - mse: 1.1684 - z0_kl_loss: 1.2331 - z_k_kl_loss: 0.0794\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2126 - mse: 0.7148 - z0_kl_loss: 1.1715 - z_k_kl_loss: 0.0774\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5814 - mse: 1.2069 - z0_kl_loss: 1.1120 - z_k_kl_loss: 0.0753\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.4534 - mse: 1.1967 - z0_kl_loss: 1.0546 - z_k_kl_loss: 0.0737\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2729 - mse: 1.1284 - z0_kl_loss: 0.9994 - z_k_kl_loss: 0.0729\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1481 - mse: 1.1103 - z0_kl_loss: 0.9469 - z_k_kl_loss: 0.0720\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.6330 - mse: 1.6963 - z0_kl_loss: 0.8978 - z_k_kl_loss: 0.0706\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2201 - mse: 1.3784 - z0_kl_loss: 0.8526 - z_k_kl_loss: 0.0682\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8957 - mse: 1.1441 - z0_kl_loss: 0.8115 - z_k_kl_loss: 0.0643\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3878 - mse: 0.7222 - z0_kl_loss: 0.7741 - z_k_kl_loss: 0.0587\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8177 - mse: 1.2340 - z0_kl_loss: 0.7394 - z_k_kl_loss: 0.0524\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.8601 - mse: 1.3528 - z0_kl_loss: 0.7065 - z_k_kl_loss: 0.0472\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5089 - mse: 1.0730 - z0_kl_loss: 0.6743 - z_k_kl_loss: 0.0436\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4542 - mse: 1.0860 - z0_kl_loss: 0.6427 - z_k_kl_loss: 0.0414\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4297 - mse: 1.1254 - z0_kl_loss: 0.6122 - z_k_kl_loss: 0.0400\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5062 - mse: 1.2614 - z0_kl_loss: 0.5836 - z_k_kl_loss: 0.0388\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6685 - mse: 1.4791 - z0_kl_loss: 0.5577 - z_k_kl_loss: 0.0370\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4941 - mse: 1.3564 - z0_kl_loss: 0.5345 - z_k_kl_loss: 0.0343\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0413 - mse: 0.9521 - z0_kl_loss: 0.5133 - z_k_kl_loss: 0.0313\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.4420 - mse: 1.3987 - z0_kl_loss: 0.4929 - z_k_kl_loss: 0.0287\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3572 - mse: 1.3570 - z0_kl_loss: 0.4726 - z_k_kl_loss: 0.0275\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8268 - mse: 0.8669 - z0_kl_loss: 0.4524 - z_k_kl_loss: 0.0276\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.6062 - mse: 1.6844 - z0_kl_loss: 0.4331 - z_k_kl_loss: 0.0278\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1041 - mse: 1.2171 - z0_kl_loss: 0.4156 - z_k_kl_loss: 0.0278\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0005 - mse: 1.1466 - z0_kl_loss: 0.4002 - z_k_kl_loss: 0.0268\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1777 - mse: 1.3550 - z0_kl_loss: 0.3858 - z_k_kl_loss: 0.0256\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1318 - mse: 1.3385 - z0_kl_loss: 0.3715 - z_k_kl_loss: 0.0251\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9846 - mse: 1.2194 - z0_kl_loss: 0.3570 - z_k_kl_loss: 0.0257\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9661 - mse: 1.2272 - z0_kl_loss: 0.3428 - z_k_kl_loss: 0.0267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81ab0fd590>"
      ]
     },
     "execution_count": 1204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model.fit(x=gid_train, \n",
    "          y=x_train, \n",
    "          batch_size=len(x_train), \n",
    "          epochs=100, \n",
    "          #verbose=False,\n",
    "          callbacks=[tf.keras.callbacks.TensorBoard(log_dir='../experiments/parameter_est/' + current_time, \n",
    "                                                    profile_batch='5',\n",
    "                                                    histogram_freq=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6012 (pid 25312), started 5:06:06 ago. (Use '!kill 25312' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d395d9c93c77edec\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d395d9c93c77edec\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6012;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=../experiments/parameter_est/ --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_estimator_error(pred, test):\n",
    "    return np.abs(pred - np.mean(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline estimator per group: 0.1178029\n",
      "Hierarchical estimator per group: 1.0413685\n",
      "Baseline estimator per group: 0.21350116\n",
      "Hierarchical estimator per group: 0.15977466\n",
      "Baseline estimator per group: 0.5193312\n",
      "Hierarchical estimator per group: 0.89544356\n"
     ]
    }
   ],
   "source": [
    "for i in range(ng):\n",
    "    train = get_segments(x_train, gid_train, i)\n",
    "    test = get_segments(x_test, gid_test, i)\n",
    "    print('Baseline estimator per group:', mean_estimator_error(train, test))\n",
    "    print('Hierarchical estimator per group:', hierarchical_estimator_error(\n",
    "        model.get_weights()[0][i,0], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline estimator per group: 0.1178029\n",
      "Hierarchical estimator per group: 0.114418134\n",
      "Baseline estimator per group: 0.21350116\n",
      "Hierarchical estimator per group: 0.2193932\n",
      "Baseline estimator per group: 0.5193312\n",
      "Hierarchical estimator per group: 0.5154686\n"
     ]
    }
   ],
   "source": [
    "for i in range(ng):\n",
    "    train = get_segments(x_train, gid_train, i)\n",
    "    test = get_segments(x_test, gid_test, i)\n",
    "    print('Baseline estimator per group:', mean_estimator_error(train, test))\n",
    "    print('Hierarchical estimator per group:', hierarchical_estimator_error(\n",
    "        model.get_weights()[0][i,0], test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.1872817],\n",
       "        [1.2238083],\n",
       "        [1.2729663]], dtype=float32), array([[-1.2659702],\n",
       "        [-1.473477 ],\n",
       "        [-1.1974527]], dtype=float32), array([-0.09241701], dtype=float32), array([3.896251], dtype=float32), array([[-0.48926044],\n",
       "        [-0.72015357],\n",
       "        [-0.59427226]], dtype=float32), array([[-2.6328142],\n",
       "        [-1.9787667],\n",
       "        [-1.7268982]], dtype=float32), 0.0, 100.0, 0.0, 100.0]"
      ]
     },
     "execution_count": 1207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultilevelDense(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, num_groups,\n",
    "                 multilevel_weights=True, \n",
    "                 multilevel_bias=True,\n",
    "                 group_kl_weights=1.,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 **kwargs):\n",
    "        super(MyMultilevelDense, self).__init__(**kwargs)\n",
    "        self.units = int(units)\n",
    "        self.num_groups = num_groups\n",
    "        self.multilevel_weights = multilevel_weights\n",
    "        self.multilevel_bias = multilevel_bias\n",
    "        self.group_kl_weights = group_kl_weights\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        print(input_shape)\n",
    "        x_input_shape, gid_input_shape = input_shape\n",
    "        last_dim = x_input_shape[-1]\n",
    "        print(self.units, last_dim)\n",
    "#         self.input_spec = [\n",
    "#             tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim}),\n",
    "#             tf.keras.layers.InputSpec(ndim=1)]\n",
    "        \n",
    "        self.w = HierarchicalVariableLayer(\n",
    "            variable_shape=[self.units, last_dim],\n",
    "            num_groups=self.num_groups, \n",
    "            kl_weight=self.group_kl_weights,\n",
    "            name='kernel')\n",
    "        \n",
    "        #if self.multilevel_bias:\n",
    "        self.b = HierarchicalVariableLayer(\n",
    "            variable_shape=[self.units],\n",
    "            num_groups=self.num_groups, \n",
    "            kl_weight=self.group_kl_weights,\n",
    "            name='bias')\n",
    "        \n",
    "        super(MyMultilevelDense, self).build(input_shape)\n",
    "     \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        \n",
    "        x, gid = inputs\n",
    "        #print(x)\n",
    "        batch_size, num_features = x.shape\n",
    "        # Sanity checks\n",
    "        #assert len(x.shape) >= 2, \"Data is incorrect shape!\"\n",
    "        #assert len(gid.shape) == 1, \"gid should be flat vector!\"\n",
    "        \n",
    "        w = self.w(gid)\n",
    "        b = self.b(gid)\n",
    "            \n",
    "        # B: batch size, p: num_features, u: num_units\n",
    "        einsum_matrix_mult = '{},Bp->Bu'.format(\n",
    "            'Bup' if self.multilevel_weights else 'up')\n",
    "        outputs = tf.einsum(einsum_matrix_mult, w, x)\n",
    "\n",
    "        # Sanity checks\n",
    "        target_shape = (batch_size, self.units)\n",
    "        msg = \"output is shape {}, when should be shape {}\".format(outputs.shape, target_shape)\n",
    "        assert outputs.shape == target_shape, msg\n",
    "        assert len(outputs.shape) == 2, \"Output is wrong shape!\"\n",
    "\n",
    "        if self.use_bias:\n",
    "            #outputs = tf.nn.bias_add(outputs, b)\n",
    "            outputs = outputs + b\n",
    "\n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 4], [3]]\n",
      "2 4\n"
     ]
    }
   ],
   "source": [
    "ml_dense = MyMultilevelDense(2, 5)\n",
    "ml_dense.build([[3,4], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 348 ms, sys: 21.6 ms, total: 370 ms\n",
      "Wall time: 376 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.00025576, -0.02346325],\n",
       "       [-0.01105966, -0.03079222],\n",
       "       [ 0.01738368,  0.06655291]], dtype=float32)>"
      ]
     },
     "execution_count": 1210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#ml_dense([tf.zeros([3], dtype=np.float32), tf.convert_to_tensor([1,1,3])])\n",
    "ml_dense([tf.zeros([3,4], dtype=np.float32), tf.convert_to_tensor([1,1,3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel/mu_k:0                                     (5, 8)\n",
      "kernel/sigma_k:0                                  (5, 8)\n",
      "kernel/mu0:0                                      (8,)\n",
      "kernel/sigma0:0                                   (8,)\n",
      "kernel/tau_k_mu:0                                 (5, 8)\n",
      "kernel/tau_k_sigma:0                              (5, 8)\n",
      "bias/mu_k:0                                       (5, 2)\n",
      "bias/sigma_k:0                                    (5, 2)\n",
      "bias/mu0:0                                        (2,)\n",
      "bias/sigma0:0                                     (2,)\n",
      "bias/tau_k_mu:0                                   (5, 2)\n",
      "bias/tau_k_sigma:0                                (5, 2)\n"
     ]
    }
   ],
   "source": [
    "for w in ml_dense.trainable_weights:\n",
    "    print('{:50}{}'.format(w.name, w.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'my_multilevel_dense_84/kernel/mu_k:0' shape=(5, 8) dtype=float32, numpy=\n",
       "array([[-0.03253335,  0.05096572,  0.02737785, -0.01956261,  0.00653011,\n",
       "        -0.0414114 ,  0.01277888, -0.04872474],\n",
       "       [ 0.02435473, -0.05468668, -0.07088862, -0.06901124, -0.07895263,\n",
       "        -0.01166569,  0.01192671, -0.00633454],\n",
       "       [-0.06163046, -0.05130154,  0.07805603, -0.09406684, -0.03157578,\n",
       "        -0.07031123,  0.1131485 ,  0.05150831],\n",
       "       [-0.03399751,  0.03623623,  0.00611739, -0.07281597,  0.01212513,\n",
       "        -0.07444499,  0.02489365, -0.00214033],\n",
       "       [-0.06717515, -0.06084746,  0.04834726, -0.04442779,  0.00446857,\n",
       "         0.05472166,  0.04395963, -0.00122406]], dtype=float32)>"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_dense.w.mu_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.9806732>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0385405>,\n",
       " <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.16555518, 0.99236894, 2.2233448 ], dtype=float32)>]"
      ]
     },
     "execution_count": 1134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90, 1], [90]]\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "ml_dense = MyMultilevelDense(1, 5)\n",
    "ml_dense.build([[90,1], [90]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 ms, sys: 1.15 ms, total: 2.99 ms\n",
      "Wall time: 1.05 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1), dtype=float32, numpy=\n",
       "array([[-0.07356545],\n",
       "       [ 0.08945476],\n",
       "       [ 0.04515195],\n",
       "       [ 0.00106587],\n",
       "       [-0.00340962],\n",
       "       [ 0.11359636],\n",
       "       [ 0.08639442],\n",
       "       [-0.00799934],\n",
       "       [ 0.06577145]], dtype=float32)>"
      ]
     },
     "execution_count": 1147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#ml_dense([tf.zeros([3], dtype=np.float32), tf.convert_to_tensor([1,1,3])])\n",
    "ml_dense([x_train, gid_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    MyMultilevelDense(units=1, num_groups=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorShape([9, 1]), TensorShape([9, 1])]\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "x = Input(shape=[1], dtype=tf.float32, batch_size=9)\n",
    "gid = Input(shape=[1], dtype=tf.int32, batch_size=9)\n",
    "mldense = MyMultilevelDense(units=1, num_groups=5)\n",
    "out = mldense([x,gid])\n",
    "model = Model(\n",
    "    inputs=[\n",
    "        #[...,tf.newaxis]\n",
    "        x, gid], \n",
    "    outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=.1), \n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 754us/step - loss: 130.5042 - mse: 16.6130 - z0_kl_loss: 8.1052 - z_k_kl_loss: 20.4947\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 679us/step - loss: 121.3732 - mse: 14.8395 - z0_kl_loss: 8.0052 - z_k_kl_loss: 19.0466\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 692us/step - loss: 113.1024 - mse: 13.2025 - z0_kl_loss: 7.9051 - z_k_kl_loss: 17.6113\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 717us/step - loss: 104.2191 - mse: 11.6693 - z0_kl_loss: 7.8050 - z_k_kl_loss: 15.9851\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 981us/step - loss: 94.6440 - mse: 10.3494 - z0_kl_loss: 7.7050 - z_k_kl_loss: 14.1114\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 760us/step - loss: 85.4424 - mse: 9.1633 - z0_kl_loss: 7.6050 - z_k_kl_loss: 12.0468\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 724us/step - loss: 79.9767 - mse: 8.1108 - z0_kl_loss: 7.5048 - z_k_kl_loss: 10.2928\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 900us/step - loss: 75.0732 - mse: 7.2088 - z0_kl_loss: 7.4037 - z_k_kl_loss: 9.8725\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 739us/step - loss: 65.8552 - mse: 6.6907 - z0_kl_loss: 7.3030 - z_k_kl_loss: 8.3642\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 710us/step - loss: 58.9841 - mse: 6.5233 - z0_kl_loss: 7.2030 - z_k_kl_loss: 5.1168\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 752us/step - loss: 66.7988 - mse: 6.3980 - z0_kl_loss: 7.1017 - z_k_kl_loss: 11.0257\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 50.4032 - mse: 6.3740 - z0_kl_loss: 7.0962 - z_k_kl_loss: 3.5423\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 872us/step - loss: 49.8008 - mse: 6.5208 - z0_kl_loss: 7.0914 - z_k_kl_loss: 3.8761\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 702us/step - loss: 47.2895 - mse: 6.7611 - z0_kl_loss: 7.0645 - z_k_kl_loss: 2.5741\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 654us/step - loss: 46.6332 - mse: 6.9114 - z0_kl_loss: 7.0339 - z_k_kl_loss: 2.5088\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 638us/step - loss: 45.9111 - mse: 6.9202 - z0_kl_loss: 7.0010 - z_k_kl_loss: 2.1634\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 664us/step - loss: 44.7320 - mse: 7.2345 - z0_kl_loss: 6.9633 - z_k_kl_loss: 2.1490\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 725us/step - loss: 43.7550 - mse: 7.2183 - z0_kl_loss: 6.9223 - z_k_kl_loss: 2.2749\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 660us/step - loss: 43.0800 - mse: 7.1244 - z0_kl_loss: 6.8792 - z_k_kl_loss: 2.3363\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 672us/step - loss: 42.5734 - mse: 7.3144 - z0_kl_loss: 6.8350 - z_k_kl_loss: 2.2809\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 849us/step - loss: 41.4642 - mse: 7.1100 - z0_kl_loss: 6.7902 - z_k_kl_loss: 2.1220\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 673us/step - loss: 40.6427 - mse: 7.2947 - z0_kl_loss: 6.7454 - z_k_kl_loss: 1.8969\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 650us/step - loss: 39.9127 - mse: 7.5489 - z0_kl_loss: 6.7010 - z_k_kl_loss: 1.6597\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 650us/step - loss: 38.4166 - mse: 6.9509 - z0_kl_loss: 6.6573 - z_k_kl_loss: 1.4667\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 706us/step - loss: 37.4271 - mse: 6.7203 - z0_kl_loss: 6.6146 - z_k_kl_loss: 1.3384\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 643us/step - loss: 36.5690 - mse: 6.5675 - z0_kl_loss: 6.5724 - z_k_kl_loss: 1.2191\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 637us/step - loss: 36.4325 - mse: 7.2170 - z0_kl_loss: 6.5298 - z_k_kl_loss: 1.0446\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 594us/step - loss: 35.1522 - mse: 6.7873 - z0_kl_loss: 6.4855 - z_k_kl_loss: 0.8317\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 615us/step - loss: 33.5112 - mse: 5.9621 - z0_kl_loss: 6.4391 - z_k_kl_loss: 0.6474\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 623us/step - loss: 33.6185 - mse: 6.7789 - z0_kl_loss: 6.3913 - z_k_kl_loss: 0.5244\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 629us/step - loss: 33.0044 - mse: 6.7055 - z0_kl_loss: 6.3425 - z_k_kl_loss: 0.4494\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 629us/step - loss: 33.0901 - mse: 7.1820 - z0_kl_loss: 6.2928 - z_k_kl_loss: 0.4077\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 917us/step - loss: 31.5928 - mse: 6.0473 - z0_kl_loss: 6.2419 - z_k_kl_loss: 0.3850\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 750us/step - loss: 31.4987 - mse: 6.3858 - z0_kl_loss: 6.1893 - z_k_kl_loss: 0.3629\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 637us/step - loss: 31.8088 - mse: 7.1662 - z0_kl_loss: 6.1345 - z_k_kl_loss: 0.3279\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 642us/step - loss: 30.1161 - mse: 5.9058 - z0_kl_loss: 6.0775 - z_k_kl_loss: 0.2784\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 636us/step - loss: 30.2717 - mse: 6.4438 - z0_kl_loss: 6.0182 - z_k_kl_loss: 0.2231\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 645us/step - loss: 29.8970 - mse: 6.4015 - z0_kl_loss: 5.9569 - z_k_kl_loss: 0.1751\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 673us/step - loss: 29.2781 - mse: 6.0733 - z0_kl_loss: 5.8939 - z_k_kl_loss: 0.1422\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 656us/step - loss: 27.9896 - mse: 5.0440 - z0_kl_loss: 5.8300 - z_k_kl_loss: 0.1258\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 674us/step - loss: 28.4236 - mse: 5.7146 - z0_kl_loss: 5.7660 - z_k_kl_loss: 0.1235\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 984us/step - loss: 28.9024 - mse: 6.4073 - z0_kl_loss: 5.7025 - z_k_kl_loss: 0.1344\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 27.9301 - mse: 5.6354 - z0_kl_loss: 5.6405 - z_k_kl_loss: 0.1525\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 989us/step - loss: 28.6305 - mse: 6.5262 - z0_kl_loss: 5.5804 - z_k_kl_loss: 0.1714\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 26.8056 - mse: 4.8742 - z0_kl_loss: 5.5225 - z_k_kl_loss: 0.1853\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 993us/step - loss: 27.0511 - mse: 5.2751 - z0_kl_loss: 5.4668 - z_k_kl_loss: 0.1931\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 751us/step - loss: 27.8892 - mse: 6.2604 - z0_kl_loss: 5.4132 - z_k_kl_loss: 0.1949\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 706us/step - loss: 26.2328 - mse: 4.7469 - z0_kl_loss: 5.3614 - z_k_kl_loss: 0.1929\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 652us/step - loss: 25.8893 - mse: 4.5198 - z0_kl_loss: 5.3110 - z_k_kl_loss: 0.1937\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 735us/step - loss: 26.3867 - mse: 5.1248 - z0_kl_loss: 5.2617 - z_k_kl_loss: 0.1982\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 629us/step - loss: 25.6541 - mse: 4.5067 - z0_kl_loss: 5.2133 - z_k_kl_loss: 0.2057\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 985us/step - loss: 26.5874 - mse: 5.5590 - z0_kl_loss: 5.1662 - z_k_kl_loss: 0.2161\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 642us/step - loss: 24.9641 - mse: 4.0497 - z0_kl_loss: 5.1207 - z_k_kl_loss: 0.2280\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 623us/step - loss: 24.9905 - mse: 4.1752 - z0_kl_loss: 5.0776 - z_k_kl_loss: 0.2405\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 635us/step - loss: 25.7370 - mse: 5.0293 - z0_kl_loss: 5.0373 - z_k_kl_loss: 0.2518\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 593us/step - loss: 24.8579 - mse: 4.2564 - z0_kl_loss: 5.0005 - z_k_kl_loss: 0.2605\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 630us/step - loss: 24.6048 - mse: 4.1009 - z0_kl_loss: 4.9672 - z_k_kl_loss: 0.2648\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 611us/step - loss: 25.1310 - mse: 4.7348 - z0_kl_loss: 4.9371 - z_k_kl_loss: 0.2648\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 791us/step - loss: 24.7353 - mse: 4.4492 - z0_kl_loss: 4.9094 - z_k_kl_loss: 0.2615\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 655us/step - loss: 24.8778 - mse: 4.6954 - z0_kl_loss: 4.8833 - z_k_kl_loss: 0.2565\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 667us/step - loss: 23.2066 - mse: 3.1316 - z0_kl_loss: 4.8578 - z_k_kl_loss: 0.2522\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 663us/step - loss: 23.2559 - mse: 3.3027 - z0_kl_loss: 4.8324 - z_k_kl_loss: 0.2490\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 755us/step - loss: 23.3306 - mse: 3.4916 - z0_kl_loss: 4.8066 - z_k_kl_loss: 0.2473\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 756us/step - loss: 23.5648 - mse: 3.8413 - z0_kl_loss: 4.7805 - z_k_kl_loss: 0.2474\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 613us/step - loss: 22.8092 - mse: 3.1938 - z0_kl_loss: 4.7544 - z_k_kl_loss: 0.2484\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 640us/step - loss: 23.4214 - mse: 3.9139 - z0_kl_loss: 4.7287 - z_k_kl_loss: 0.2495\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 620us/step - loss: 23.8581 - mse: 4.4563 - z0_kl_loss: 4.7038 - z_k_kl_loss: 0.2497\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 841us/step - loss: 23.3444 - mse: 4.0465 - z0_kl_loss: 4.6799 - z_k_kl_loss: 0.2489\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 669us/step - loss: 22.8734 - mse: 3.6693 - z0_kl_loss: 4.6572 - z_k_kl_loss: 0.2479\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 637us/step - loss: 22.4219 - mse: 3.3094 - z0_kl_loss: 4.6352 - z_k_kl_loss: 0.2480\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 718us/step - loss: 21.7443 - mse: 2.7265 - z0_kl_loss: 4.6137 - z_k_kl_loss: 0.2493\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 630us/step - loss: 21.6140 - mse: 2.6895 - z0_kl_loss: 4.5923 - z_k_kl_loss: 0.2511\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 22.1420 - mse: 3.3147 - z0_kl_loss: 4.5709 - z_k_kl_loss: 0.2532\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 645us/step - loss: 21.1418 - mse: 2.4067 - z0_kl_loss: 4.5496 - z_k_kl_loss: 0.2552\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 627us/step - loss: 21.7181 - mse: 3.0724 - z0_kl_loss: 4.5287 - z_k_kl_loss: 0.2569\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 740us/step - loss: 21.5598 - mse: 2.9985 - z0_kl_loss: 4.5084 - z_k_kl_loss: 0.2583\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 652us/step - loss: 21.4767 - mse: 2.9883 - z0_kl_loss: 4.4889 - z_k_kl_loss: 0.2588\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 700us/step - loss: 20.4485 - mse: 2.0291 - z0_kl_loss: 4.4701 - z_k_kl_loss: 0.2589\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 661us/step - loss: 20.8222 - mse: 2.4739 - z0_kl_loss: 4.4517 - z_k_kl_loss: 0.2589\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 650us/step - loss: 21.4565 - mse: 3.1740 - z0_kl_loss: 4.4336 - z_k_kl_loss: 0.2583\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 747us/step - loss: 21.5356 - mse: 3.3201 - z0_kl_loss: 4.4155 - z_k_kl_loss: 0.2571\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 630us/step - loss: 21.0635 - mse: 2.9061 - z0_kl_loss: 4.3973 - z_k_kl_loss: 0.2561\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 625us/step - loss: 20.5200 - mse: 2.4196 - z0_kl_loss: 4.3790 - z_k_kl_loss: 0.2555\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 659us/step - loss: 20.0990 - mse: 2.0523 - z0_kl_loss: 4.3608 - z_k_kl_loss: 0.2556\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 718us/step - loss: 21.4114 - mse: 3.4137 - z0_kl_loss: 4.3431 - z_k_kl_loss: 0.2551\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 665us/step - loss: 20.1525 - mse: 2.2001 - z0_kl_loss: 4.3262 - z_k_kl_loss: 0.2545\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 637us/step - loss: 19.5707 - mse: 1.6660 - z0_kl_loss: 4.3103 - z_k_kl_loss: 0.2536\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 721us/step - loss: 19.7651 - mse: 1.9123 - z0_kl_loss: 4.2954 - z_k_kl_loss: 0.2520\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 712us/step - loss: 20.0501 - mse: 2.2588 - z0_kl_loss: 4.2813 - z_k_kl_loss: 0.2502\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 617us/step - loss: 19.5873 - mse: 1.8608 - z0_kl_loss: 4.2678 - z_k_kl_loss: 0.2484\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 625us/step - loss: 19.8947 - mse: 2.2283 - z0_kl_loss: 4.2548 - z_k_kl_loss: 0.2466\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 616us/step - loss: 19.2883 - mse: 1.6784 - z0_kl_loss: 4.2419 - z_k_kl_loss: 0.2445\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 623us/step - loss: 20.4971 - mse: 2.9464 - z0_kl_loss: 4.2288 - z_k_kl_loss: 0.2416\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 638us/step - loss: 19.7650 - mse: 2.2659 - z0_kl_loss: 4.2156 - z_k_kl_loss: 0.2394\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 898us/step - loss: 19.5959 - mse: 2.1542 - z0_kl_loss: 4.2024 - z_k_kl_loss: 0.2373\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 672us/step - loss: 19.6193 - mse: 2.2304 - z0_kl_loss: 4.1893 - z_k_kl_loss: 0.2358\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 748us/step - loss: 19.1089 - mse: 1.7711 - z0_kl_loss: 4.1766 - z_k_kl_loss: 0.2337\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 636us/step - loss: 19.0603 - mse: 1.7751 - z0_kl_loss: 4.1641 - z_k_kl_loss: 0.2307\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 626us/step - loss: 18.9488 - mse: 1.7126 - z0_kl_loss: 4.1519 - z_k_kl_loss: 0.2272\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 716us/step - loss: 19.2706 - mse: 2.0854 - z0_kl_loss: 4.1398 - z_k_kl_loss: 0.2247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f819ad57590>"
      ]
     },
     "execution_count": 1214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[x_train, gid_train], \n",
    "          y=(x_train*0.1+4.), \n",
    "          batch_size=9, \n",
    "          epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel/mu_k:0\n",
      "[[0.66215134]\n",
      " [0.7480566 ]\n",
      " [0.7280165 ]\n",
      " [0.65358394]\n",
      " [0.6598954 ]]\n",
      "\n",
      "\n",
      "kernel/sigma_k:0\n",
      "[[-1.757535 ]\n",
      " [-0.7219534]\n",
      " [-1.0558423]\n",
      " [-1.3197732]\n",
      " [-1.8167602]]\n",
      "\n",
      "\n",
      "kernel/mu0:0\n",
      "[1.8726854]\n",
      "\n",
      "\n",
      "kernel/sigma0:0\n",
      "[-0.07809245]\n",
      "\n",
      "\n",
      "kernel/tau_k_mu:0\n",
      "[[-0.3865319 ]\n",
      " [-0.70876133]\n",
      " [-0.8333371 ]\n",
      " [-0.46348873]\n",
      " [-0.45331126]]\n",
      "\n",
      "\n",
      "kernel/tau_k_sigma:0\n",
      "[[-2.8097882]\n",
      " [-1.8361427]\n",
      " [-2.132895 ]\n",
      " [-2.6548042]\n",
      " [-1.5749625]]\n",
      "\n",
      "\n",
      "bias/mu_k:0\n",
      "[[1.8920902]\n",
      " [1.5902741]\n",
      " [1.5186992]\n",
      " [1.0572479]\n",
      " [1.4578956]]\n",
      "\n",
      "\n",
      "bias/sigma_k:0\n",
      "[[ 0.35088223]\n",
      " [-0.87024796]\n",
      " [-1.2942924 ]\n",
      " [-0.22717942]\n",
      " [-0.97404236]]\n",
      "\n",
      "\n",
      "bias/mu0:0\n",
      "[0.19788465]\n",
      "\n",
      "\n",
      "bias/sigma0:0\n",
      "[-0.02248747]\n",
      "\n",
      "\n",
      "bias/tau_k_mu:0\n",
      "[[-1.2978233 ]\n",
      " [-0.54966015]\n",
      " [-0.5668746 ]\n",
      " [-0.70385695]\n",
      " [-0.6371387 ]]\n",
      "\n",
      "\n",
      "bias/tau_k_sigma:0\n",
      "[[-2.3439867]\n",
      " [-2.6224136]\n",
      " [-1.7042128]\n",
      " [-2.2351463]\n",
      " [-1.7297426]]\n",
      "\n",
      "\n",
      "kernel/z0_prior_mean:0\n",
      "0.0\n",
      "\n",
      "\n",
      "kernel/z0_prior_variance:0\n",
      "100.0\n",
      "\n",
      "\n",
      "kernel/tau_k_prior_mean:0\n",
      "0.0\n",
      "\n",
      "\n",
      "kernel/tau_k_prior_variance:0\n",
      "100.0\n",
      "\n",
      "\n",
      "bias/z0_prior_mean:0\n",
      "0.0\n",
      "\n",
      "\n",
      "bias/z0_prior_variance:0\n",
      "100.0\n",
      "\n",
      "\n",
      "bias/tau_k_prior_mean:0\n",
      "0.0\n",
      "\n",
      "\n",
      "bias/tau_k_prior_variance:0\n",
      "100.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for w in model.weights:\n",
    "    print(w.name) \n",
    "    print(w.numpy())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
